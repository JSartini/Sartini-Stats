{
  "hash": "c2ceddebf44c4032c522670606dbccbb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"FAST: Bayesian FPCA Vignette\"\ndate: 11/24/2024\neditor: source\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"FAST_support/Libs.R\")\nsource(\"FAST_support/Bases.R\")\nsource(\"FAST_support/Convergence.R\")\nsource(\"FAST_support/PostProcess.R\")\nsource(\"FAST_support/FAST_Help.R\")\n```\n:::\n\n\n## Read in the NHANES Accelerometry Data\n\nFor this brief example analysis, we leverage the objective physical activity data from wrist-worn accelerometry collected as part of NHANES 2011-2014. This dataset is made publicly available as part of [\"Functional Data Analysis with R\"](https://functionaldataanalysis.org) and can be downloaded from the accompanying website. We take a random subsample of 200 individuals subsampled every 10 minutes during the day (144 total measurements) for the sake of time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_path = \"http://www.ciprianstats.org/sites/default/files/nhanes/nhanes_fda_with_r.rds\"\ndownload.file(data_path, \"nhanes.rds\", mode = \"wb\")\naccel_data = readRDS(\"nhanes.rds\")\nfile.remove(\"nhanes.rds\")\n\nset.seed(12345)\nchosen_idxs = sample(1:nrow(accel_data), 200)\naccel_mat = unAsIs(accel_data$MIMS)[chosen_idxs, ]\naccel_df = data.frame(accel_mat)\ncolnames(accel_df) = 1:1440\naccel_df$ID = accel_data$SEQN[chosen_idxs]\naccel_df = accel_df %>%\n  pivot_longer(-c(ID), names_to = \"MoD\", values_to = \"MIMS\") %>%\n  mutate(MoD = as.numeric(MoD), \n         Window = (MoD - 1) %/% 10) %>%\n  group_by(ID, Window) %>%\n  summarize(MIMS = mean(MIMS)) %>%\n  mutate(MoD = Window * 10)\n```\n:::\n\n\n## Visualization of Data Structure\n\nNow that we have the data, we perform quick visualizations to get an idea of the types of patterns present. We first randomly select 5 participant's MIMS (monitor-independent movement summary) curves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0935867)\nchosen_ids = sample(unique(accel_df$ID), 6)\n\naccel_df %>%\n  filter(ID %in% chosen_ids) %>%\n  mutate(HoD = MoD/60, ID = paste0(\"ID: \", ID)) %>%\n  ggplot(aes(x = HoD, y = MIMS, group = ID)) + \n  geom_line() +\n  facet_wrap(.~ID) + \n  theme_bw() + \n  labs(x = \"Hour of the Day\", y = \"MIMS\")\n```\n\n::: {.cell-output-display}\n![](FAST_Vignette_files/figure-html/data_vis_subset-1.png){width=672}\n:::\n:::\n\n\nWe can alternatively visualize the entire population using a heatmap structure, where each row is a participant and darker colors correspond to higher activity levels (as in [\"Functional Data Analysis with R\"](https://functionaldataanalysis.org)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccel_df %>%\n  mutate(ID = factor(ID), \n         HoD = MoD/60) %>%\n  ggplot(aes(x = HoD, y = ID)) + \n  geom_tile(aes(fill = MIMS)) +\n  theme_bw() + \n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank()) +\n  scale_fill_gradient(low = \"white\", high = \"black\") + \n  labs(x = \"Hour of the Day\", y = \"Participant\")\n```\n\n::: {.cell-output-display}\n![](FAST_Vignette_files/figure-html/data_vis_full-1.png){width=672}\n:::\n:::\n\n\nThis visualization indicates that activity is strongest between the hours of 8AM and 10PM, as would be expected. However, there is substantial heterogeneity both in when individuals start and end their days, as well as the amount of activity they engage in on average throughout the course of the day.\n\n## Fitting FAST Bayesian FPCA\n\nWe first retrieve the matrix representation of the full dataset using the appropriate wrangling. We then define the constants, using spline basis of dimension $Q = 25$ and $K = 3$ FPCs. We provide these values, along with the input data matrix, to the \"FAST_datalist()\" function, which concludes collating all requisite inputs to the FAST STAN implementation. This includes generating the spline bases for the fixed and random effects functions (b-splines and orthogonal splinets, respectively), along with their associated quadratic penalty matrices. All of these elements are collated into the STAN input list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Place data in wide matrix format\nY_mat = accel_df %>% \n  ungroup() %>%\n  select(-c(Window)) %>%\n  pivot_wider(names_from = MoD, values_from = MIMS) %>%\n  select(-c(ID)) %>% as.matrix()\n\n# Define constants\nQ = 25\nK = 3\n\n# Collate list of arguments\nDomain = sort(unique(accel_df$MoD))/60\nScaled_Domain = (Domain - min(Domain))/(max(Domain) - min(Domain))\ndata_list = FAST_datalist(Y_mat, Q, K, Scaled_Domain)\n```\n:::\n\n\nWith all requisite inputs generated, we can finally make the call to RSTAN in order to fit the model, accomplished as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_mod = stan(file = \"FAST_support/FAST.stan\",\n               data = data_list, \n               chains = 4, \n               cores = 4, \n               warmup = 3000, \n               iter = 5000)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Evaluating Fit and Aligning Results\n\nWe first extract and summarize all relevant model parameters, aligning the FPCs and scores according to sign and order. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nobjects = FAST_extract(fit_mod, data_list$B_FE, \n                       data_list$B_RE, Domain, data_list)\n\nalign = align_weights(objects$Weights, objects$Score, data_list$B_RE)\nscores_samples = out_Score(align$Score)\n    \nEF_CI = CI_EF(align$EF, Domain)\nEF_est = Psi_SVD_FPC(align$Weights, data_list$B_RE, Domain, K)\nMu_df = out_FE(objects$Mu, Domain) %>%\n  group_by(Arg) %>%\n  summarize(Est = mean(Mu), \n            LB = quantile(Mu, probs = c(0.025)), \n            UB = quantile(Mu, probs = c(0.975)))\n```\n:::\n\n\nThe next step is assessment of model convergence using Gelman-Rubin RHat statistics. Principal component sign and ordering are aligned prior to calculating these sampling diagnostics. These measures are calculated using the \"RHat_FAST()\" function as follows. This function calculates the median and max RHat observed for each FPC/covariate grouping within parameter families. The results below indicate that all parameters have converged according to the heuristic RHat < 1.05 threshold. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrhats = RHat_FAST(fit_mod, data_list, align$EF[[1]])\n\n# FPCs, grouped by FPC/functional component\nrhats$Func\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  Function Med_RHat Max_RHat\n  <chr>       <dbl>    <dbl>\n1 FPC 1        1.02     1.02\n2 FPC 2        1.02     1.02\n3 FPC 3        1.00     1.00\n4 Mu           1.01     1.01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scores, grouped by FPC\nrhats$Score\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  FPC_Num Med_RHat Max_RHat\n  <chr>      <dbl>    <dbl>\n1 FPC 1       1.01     1.03\n2 FPC 2       1.01     1.02\n3 FPC 3       1.00     1.01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Smoothing parameters, grouped by functional component\nrhats$Smoothing_Params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Function      RHat\n1    FPC 1 1.0002062\n2    FPC 2 0.9997524\n3    FPC 3 0.9997878\n4       Mu 0.9997878\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance components (eigenvalues and noise variance)\nrhats$Variances\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Element     RHat\n1 Lambda_1 1.007618\n2 Lambda_2 1.012016\n3 Lambda_3 1.000078\n4   Sigma2 1.000626\n```\n\n\n:::\n:::\n\n\nTo evaluate the adequacy of $K = 3$ principal components, we evaluate the posterior distribution of variability explained. Results indicate that $K = 3$ principal components explains $\\approx 80$% of the variability in the observed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# True data in sparse matrix form\ndata_var = var(as.vector(Y_mat))\n\n# Modeled smooths in sparse matrix of dim. samples x ppts x obs\nsamples_matrix = Smooth_Raw(objects$Mu, align$EF, align$Score, data_list)\nn_samp = dim(samples_matrix)[1]\nvar_expl = rep(0, n_samp)\nfor (j in 1:n_samp) {\n  model_matrix = samples_matrix[j, , ]\n  resids = Y_mat - model_matrix\n  var_expl[j] = 1 - var(as.vector(resids)) / data_var # 1 - RSS/TSS across covariates\n}\n\n# Summarize global variance explained over posterior samples\nsummary(var_expl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7803  0.7811  0.7813  0.7813  0.7815  0.7822 \n```\n\n\n:::\n:::\n\n\n## Visualizing Principal Components\n\nWith FAST fit, we can now visualize the extracted and aligned model components: fixed effects function $\\mu(t)$, eigenfunctions $\\phi_k(t)$, and eigenvalues $\\lambda_k$. We first visualize the fixed effects mean $\\mu(t)$, displaying the posterior mean estimate, equal-tail 95% credible interval, and 3 posterior samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_samples = sample(length(objects$Mu), 3)\nfunc_samples = map(chosen_samples, function(idx){\n  out_df = data.frame(Mu = objects$Mu[[idx]],\n                      Arg = Domain, Sample = idx)\n  return(out_df)\n}) %>% list_rbind()\n\nMu_df %>%\n  ggplot(aes(x = Arg)) + \n  geom_line(aes(y = Est)) + \n  geom_ribbon(aes(ymin = LB, ymax = UB), alpha = 0.1) + \n  geom_line(data = func_samples, aes(y = Mu, group = Sample), color = \"red\", alpha = 0.5) +\n  theme_bw() + \n  labs(x = \"Hour of the Day\", y = parse(text = \"mu(t)~Estimate\"))\n```\n\n::: {.cell-output-display}\n![](FAST_Vignette_files/figure-html/fe_vis-1.png){width=672}\n:::\n:::\n\n\nWe can similarly visualize the samples of eigenfunctions $\\phi_k(t)$, again including the estimate, equal-tail 95% credible interval, and 3 posterior samples in red.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunc_sample = map(chosen_samples, function(idx){\n  out_df = FPC_df(align$EF[[idx]], Domain) %>%\n    mutate(Sample = idx, \n           EFLab = paste0(\"phi[\", substring(FPC_Num, 4), \"](t)\"))\n  return(out_df)\n}) %>% list_rbind()\n\nleft_join(EF_est, EF_CI) %>%\n  mutate(EFLab = paste0(\"phi[\", substring(FPC_Num, 4), \"](t)\")) %>%\n  ggplot(aes(x = Arg)) + \n  geom_line(aes(y = FPC_Val)) +\n  geom_ribbon(aes(ymin = LB, ymax = UB), alpha = 0.1) + \n  geom_line(data = func_sample, aes(y = FPC_Val, group = Sample), color = \"red\", alpha = 0.5) +\n  facet_wrap(.~EFLab, labeller = label_parsed) + \n  theme_bw() + \n  labs(x = \"Hour of the Day\", y = \"Eigenfunction Estimate\")\n```\n\n::: {.cell-output-display}\n![](FAST_Vignette_files/figure-html/ef_vis-1.png){width=1152}\n:::\n:::\n\n\nWe can also evaluate the variability decomposition by plotting the estimated eigenvalues $\\lambda_k$ sequentially with their equal-tail 95% credible intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlabs = paste0(\"lambda[\", 1:K, \"]\")\nnames(labs) = paste0(\"FPC \", 1:K)\n\nmap(1:length(align$Score), function(x){\n  eigenvals = apply(align$Score[[x]], 2, var)\n  return(data.frame(FPC = paste0(\"FPC \", 1:K), \n                    EVal = eigenvals))\n}) %>% list_rbind() %>%\n  group_by(FPC) %>%\n  summarize(Est = mean(EVal),\n            LB = quantile(EVal, probs = c(0.025)), \n            UB = quantile(EVal, probs = c(0.975))) %>%\n  ggplot(aes(x = FPC, y = Est, group = \"\")) +\n  geom_point() + \n  geom_line() + \n  geom_errorbar(aes(ymin = LB, ymax = UB), width = 0.5) + \n  scale_x_discrete(labels = parse(text = labs)) + \n  labs(y = \"Eigenvalue\") + \n  theme_bw() + \n  theme(axis.title.x = element_blank())\n```\n\n::: {.cell-output-display}\n![](FAST_Vignette_files/figure-html/ev_vis-1.png){width=672}\n:::\n:::\n\n\nThis document just visualizes a particular case of applying the \"[FAST](../resources/publications/pre_print/sartini_BFPCA_2024.qmd)\" approach to perform fully-Bayesian Functional PCA. All supporting STAN and R code here is designed to be flexible, such that it may directly be applied to any other similarly structured problem. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}