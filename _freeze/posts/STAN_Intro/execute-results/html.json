{
  "hash": "6df0ebc93e287168a4842be48ed3b86a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"STAN Introduction\"\nauthor: Joseph Sartini\ndate: 01/31/2025\nformat: \n  revealjs:\n    theme: simple\n    self-contained: false\n    auto-stretch: true\neditor: source\n---\n\n::: {.cell}\n\n:::\n\n\n\n## What is STAN?\n\n::: {}\n- Bayesian probabilistic programming language\n\n- Multiple posterior sampling routines\n\n    - Hamiltonian Monte Carlo\n    \n    - Variational Inference\n    \n    - Laplace approximation\n\n- Based on C++\n\n- Interfaces with Python, Julia, R, and Unix Shell\n:::\n\n## Structure of a STAN Script\n\n\n::: {.cell output.var='blank'}\n\n```{.stan .cell-code}\nfunctions {\n  // ... function declarations and definitions ...\n}\ndata {\n  // ... declarations ...\n}\ntransformed data {\n   // ... declarations ... statements ...\n}\nparameters {\n   // ... declarations ...\n}\ntransformed parameters {\n   // ... declarations ... statements ...\n}\nmodel {\n   // ... declarations ... statements ...\n}\ngenerated quantities {\n   // ... declarations ... statements ...\n}\n```\n:::\n\n\n::: notes\nAll blocks are optional, an empty string is actually a valid STAN code.\nOrder is important, they must be in the above order\nVariables have scope over all subsequent blocks\n:::\n\n## Section - functions\n\n::: {}\n- Complex indexing\n\n    - Sparsely observed data\n\n- Generating quantities/structures\n\n    - Splines, etc.\n\n- Suffixes for particular functions\n    \n    - Containing RNG: \"_rng\"\n    \n    - Modifying target density: \"_lp\" \n:::\n\n::: notes\nRNG functions can only be used on data/generated quantities\n:::\n\n## Section - data\n\n::: {}\n- Likelihood data\n\n    - Indexing arrays\n\n- All constants\n\n    - Array extents\n    \n- Commonly used linear transforms\n:::\n\n::: notes\nOften need more information in this block than you think to fully specify the model.\n:::\n\n## Section - transformed data\n\n::: {}\n- Functions of data variables\n\n- Only evaluated once\n\n    - Prior to sampling\n    \n- Helpful for book-keeping\n\n    - Simplify data inputs\n\n- Random data sub-samples\n:::\n\n## Section - parameters\n\n::: {}\n- Specify sampled quantities\n\n    - Variable names\n    \n    - Extents\n\n- Definitions only, no statements\n\n- Read from underlying sampler\n\n- Can provide initial values\n:::\n\n::: notes\n- Will get more into how to provide initial values shortly\n:::\n\n## Section - transformed parameters\n\n::: {}\n- Deterministic functions\n    \n- Part of target posterior\n\n- Evaluated with each sample\n\n    - Inverse transform\n    \n    - Log absolute Jacobian\n    \n- Good for re-parameterization\n\n    - Stability \n    \n    - Latent modeling\n:::\n\n::: notes\n- Can include data, transformed data, and parameters as inputs\n\n- Need inverse transform and log absolute Jacobian to be efficient, as they can greatly slow down the sampling process otherwise\n\n- Natural scale vs sampling efficiency (NUTS and standard HMC prefer unconstrained parameters). However, gradient calculations usually dwarf these differences\n:::\n\n## Section - model\n\n::: {}\n- Define the target posterior\n\n    - Sum of log density functions\n\n- Prior distributions on (transformed) parameters\n\n- Data/model likelihood\n\n- Most computational expense\n\n- ORDER MATTERS\n:::\n\n::: notes\nIt is often important to know which subgroup of operations within the model block is consuming the majority of computation time (after deciding that efficient is insufficient). We will get to how this can be done (do not optimize unless necessary).\n:::\n\n## Section - generated quantities\n\n::: {}\n- Executed after samples are generated\n\n- Functions of model output\n\n    - Predictions for new data\n    \n    - Simulate new data\n    \n    - Extract posterior estimates\n    \n    - Calculate model fit criterion\n:::\n\n## Example Model - GLM\n\n\n::: {.cell output.var='first_model'}\n\n```{.stan .cell-code}\ndata {\n  int N;  // Number of observations\n  int P;  // Number of fixed effect covariates\n  \n  array[N] int<lower=0, upper=1> Y;  // Binary outcomes\n  matrix[N, P] X;                    // Fixed effects design matrix\n}\nparameters {\n   vector[P] beta;  // Coefficients\n}\nmodel {\n   Y ~ bernoulli_logit(X * beta);\n}\n```\n:::\n\n\n::: notes\nWhile there is no explicit prior on beta here, that just means that STAN is choosing the equivalent to a uniform prior under the hood\n:::\n\n## Running the Model in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_df = mtcars %>%\n  mutate(Efficient = case_when(mpg >= median(mpg) ~ 1,\n                               TRUE ~ 0)) %>%\n  mutate(am = as.factor(am))\nfit_matrix = model.matrix(~cyl + disp + hp + drat + wt + am, fit_df)\n\ndata_list = list(N = nrow(mtcars), P = ncol(fit_matrix), \n                 Y = fit_df$Efficient, X = fit_matrix)\n\nmodel = sampling(\n  first_model, \n  data = data_list, \n  chains = 4, \n  iter = 1000, \n  warmup = 500, \n  # init = ,\n  # control = list(adapt_delta = , \n  #                max_treedepth = , \n  #                stepsize_jitter = ), \n  verbose = F,\n  refresh = 0\n)\n```\n:::\n\n\n## Convergence Monitoring\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_hmc_diagnostics(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDivergences:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n0 of 2000 iterations ended with a divergence.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTree depth:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n1246 of 2000 iterations saturated the maximum tree depth of 10 (62.3%).\nTry increasing 'max_treedepth' to avoid saturation.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEnergy:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nE-BFMI indicated no pathological behavior.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$summary[,\"Rhat\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n beta[1]  beta[2]  beta[3]  beta[4]  beta[5]  beta[6]  beta[7]     lp__ \n1.515704 1.669643 1.563934 1.582427 1.574460 1.856891 1.557981 1.051830 \n```\n\n\n:::\n:::\n\n\n## Hamiltonian Monte Carlo\n\n![HMC Visualization: By Justinkunimune - github.com/jkunimune/hamiltonian-mc, CC0](../resources/images/Hamiltonian_Monte_Carlo.gif)\n\n## Hamiltonian Monte Carlo Continued\n\n::: {}\n- Version of Metropolis-Hastings\n\n- Hamiltonian Dynamics used to propose next state\n\n    - Trajectory with momentum\n    \n    - Distribution $\\approx$ potential energy field\n    \n    - Leapfrog integrator stepwise approximation\n\n- Momentum: reduced correlation between samples\n\n- Energy conservation: high acceptance probability\n:::\n\n::: notes\n- Leapfrog integrator uses model gradient to take steps, why it is important to keep this efficient (analytical gradients built-in by STAN for many things)\n\n- Use MH step to compensate for numerical issues in the Leapfrog algorithm, but acceptance probability should be high if things go well (not needed in perfect world)\n:::\n\n## Divergences\n\n::: {}\n- Simulated trajectory $\\neq$ true trajectory\n\n- Global step size $>$ true posterior geometry resolution\n\n    - Leapfrog first order approximataion\n\n- Hamiltonian departs from initial value\n\n    - Total energy (kinetic + potential)\n    \n    - Should be preserved along trajectory\n    \n- Sampler WILL NOT accept samples after divergence\n:::\n\n## Tree Depth Warnings\n\n::: {}\n- Tree depth controls number of simulation steps\n\n    - $\\leq 2^{max\\_treedepth}$ steps\n    \n- Primarily an efficiency concern\n\n- Generally recommended to not increase\n\n    - Often model misspecification\n:::\n\n## Est. Bayesian Fraction of Miss. Info.\n\n::: {}\n- Posterior decomposes into energy equivalence classes\n\n- Low EBFMI indicates getting \"stuck\" in energy sets\n\n     - STAN monitors energy during sampling\n     \n- Insufficiently exploring the posterior\n\n    - Tails too large, etc\n:::\n\n## Geometric Intuition\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](STAN_Intro_files/figure-revealjs/intuition_figure-1.png){width=960}\n:::\n:::\n\n\n## Return to our Example: Model Outputs {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples = extract(model)\n\nbeta_0 = map(1:dim(samples$beta)[1], function(x){\n  return(data.frame(beta0 = samples$beta[x,1], \n                    Sample = x))\n}) %>% list_rbind()\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](STAN_Intro_files/figure-revealjs/pma_vis-1.png){width=960}\n:::\n:::\n\n\n\n## Model Outputs (2) {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples = extract(model, \"beta\", permuted = FALSE)\n\nbeta_0 = map(1:dim(samples)[1], function(x){\n  return(data.frame(beta0 = samples[x,,1], \n                    Chain = 1:4, \n                    sample = x))\n}) %>% list_rbind()\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](STAN_Intro_files/figure-revealjs/sms_vis-1.png){width=960}\n:::\n:::\n\n\n## ShinySTAN Debugging\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlaunch_shinystan(model)\n```\n:::\n\n\n![](../resources/images/ShinySTAN.png){fig-align=\"center\"}\n\n## ShinySTAN - NUTS Summary\n\n![](../resources/images//Diagnose_NUTS.png){fig-align=\"center\"}\n\n## ShinySTAN - Divergences\n\n![](../resources/images//Diagnose_Diverge.png){fig-align=\"center\"}\n\n## ShinySTAN - Treedepth\n\n![](../resources/images//Diagnose_Treedepth.png){fig-align=\"center\"}\n\n## ShinySTAN - Energy\n\n![](../resources/images//Diagnose_Energy.png){fig-align=\"center\"}\n\n## ShinySTAN - Autocorrelation\n\n![](../resources/images//Autocorrelation.png){fig-align=\"center\"}\n\n## How to Update the Model\n\n\n::: {.cell output.var='second_model'}\n\n```{.stan .cell-code}\ndata {\n  int N;  // Number of observations\n  int P;  // Number of fixed effect covariates\n  \n  array[N] int<lower=0, upper=1> Y;  // Binary outcomes\n  matrix[N, P] X;                    // Fixed effects design matrix\n}\ntransformed data {\n  matrix[N, P] Q_coef = qr_thin_Q(X) * sqrt(N-1);\n  matrix[P, P] R_coef = qr_thin_R(X) / sqrt(N-1);\n  matrix[P, P] R_coef_inv = inverse(R_coef);\n}\nparameters {\n  vector[P] theta;  // Coefficients\n}\nmodel {\n  theta ~ normal(0, 100);\n  Y ~ bernoulli_logit(Q_coef * theta);\n}\ngenerated quantities {\n  vector[P] beta = R_coef_inv * theta;\n}\n```\n:::\n\n\n## Running the Updated Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = sampling(\n  second_model, \n  data = data_list, \n  chains = 4, \n  iter = 2500, \n  warmup = 1000, \n  control = list(adapt_delta = 0.95),\n  verbose = F,\n  refresh = 0\n)\n```\n:::\n\n\n## Updated Model Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_hmc_diagnostics(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDivergences:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n0 of 6000 iterations ended with a divergence.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTree depth:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n0 of 6000 iterations saturated the maximum tree depth of 10.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nEnergy:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nE-BFMI indicated no pathological behavior.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$summary[,\"Rhat\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntheta[1] theta[2] theta[3] theta[4] theta[5] theta[6] theta[7]  beta[1] \n1.022641 1.022835 1.009543 1.022226 1.022812 1.019794 1.022354 1.017068 \n beta[2]  beta[3]  beta[4]  beta[5]  beta[6]  beta[7]     lp__ \n1.006945 1.023051 1.022486 1.022930 1.002514 1.022354 1.005698 \n```\n\n\n:::\n:::\n\n\n## Updated Model Visualization {.scrollable}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](STAN_Intro_files/figure-revealjs/vis_v2-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](STAN_Intro_files/figure-revealjs/vis_v2-2.png){width=960}\n:::\n:::\n\n\n## Updated Geometry\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](STAN_Intro_files/figure-revealjs/intuition_update-1.png){width=960}\n:::\n:::\n\n\n\n## Profiling the Model with CmdStanR (1)\n\n\n::: {.cell output.var='Profile_Mod'}\n\n```{.stan .cell-code}\ndata {\n  int N;  // Number of observations\n  int P;  // Number of fixed effect covariates\n  \n  array[N] int<lower=0, upper=1> Y;  // Binary outcomes\n  matrix[N, P] X;                    // Fixed effects design matrix\n}\ntransformed data {\n  matrix[N, P] Q_coef = qr_thin_Q(X) * sqrt(N-1);\n  matrix[P, P] R_coef = qr_thin_R(X) / sqrt(N-1);\n  matrix[P, P] R_coef_inv = inverse(R_coef);\n}\nparameters {\n  vector[P] theta;  // Coefficients\n}\nmodel {\n  profile(\"Priors\") {\n    target += normal_lpdf(theta| 0, 100);\n  }\n  profile(\"Likelihood\") {\n    target += bernoulli_logit_lpmf(Y| Q_coef * theta);\n  }\n}\ngenerated quantities {\n  profile(\"Generated\") {\n    vector[P] beta = R_coef_inv * theta;\n  }\n}\n```\n:::\n\n\n## Profiling the Model with CmdStanR (2)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = cmdstan_model(\"Profile_Mod.stan\")\nfit = model$sample(data = data_list, chains = 1)\nfit$profiles()[[1]][,c(1,3,4,5,8)]\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n        name total_time forward_time reverse_time autodiff_calls\n1  Generated 0.00015283   0.00015283   0.00000000              0\n2 Likelihood 0.13750200   0.10993300   0.02756940         170575\n3     Priors 0.02505130   0.02181140   0.00323987         170575\n```\n\n\n:::\n:::\n\n\n## Resource Links\n\n- [STAN Manual](https://mc-stan.org/docs/reference-manual/)\n\n- [Introduction to HMC](https://arxiv.org/pdf/1701.02434)\n\n- [Bayesian Workflow](https://arxiv.org/pdf/2011.01808)\n\n::: notes\nIntro to HMC by Michael Betancourt\n\nBayesian Workflow by Gelman et. al.\n:::",
    "supporting": [
      "STAN_Intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}