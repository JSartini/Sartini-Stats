{
  "hash": "fffb3dd88b3c9618d7391648c9e66b2e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Fully-Bayesian Functional Modeling of Continuous Glucose Monitoring Data\"\nauthor:\n  name: Joseph Sartini, PhD Candidate\n  affiliation: Johns Hopkins University\nformat: \n  revealjs:\n    theme: [default, custom.scss]\n    embed-resources: true\n    auto-stretch: false\n    self-contained: true\n    include-in-header:\n      - text: |\n          <style>\n            .reveal p > img {\n              margin: 0;\n            }\n          </style>\n          <style>\n          .overflow-image {\n            max-width: none !important;\n            position: absolute;\n            left: 50%;\n            transform: translateX(-50%);\n          }\n          </style>\neditor: source\nbibliography: refs.bib\ncsl: apa.csl\n---\n\n\n## Agenda\n\n:::{style=\"font-size: 1.3em;\"}\n- Modeling Continuous Glucose Monitor (CGM) Data\n\n- A Bayesian Functional Model for CGM Data\n\n- Model Performance\n\n- Model Inference on CGM\n\n- Extending our approach\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Introduce the scientific problem(s) we hope to address\n\n- Modeling approach to leverage this data\n\n- How does the approach do in objective evaluation\n\n- Applying the approach to our data\n\n- Can the approach be generalized more broadly?\n\nTransition: With that said, let's get started\n:::\n\n## Public Health Perspective: Diabetes\n\n::: {.absolute top=\"15%\" left=\"15%\" width=\"700px\" height=\"auto\"}\n\n::: {style=\"margin-bottom: 0px\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/cdc_fig-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.r-stack style=\"margin-top: 0px; font-size: 0.45em;\"}\nEstimates collated by the [CDC](https://gis.cdc.gov/grasp/diabetes/diabetesatlas-statsreport.html) using data from the National Heath and Nutrition Examination Survey.\n:::\n\n:::\n\n::: {.absolute top=\"66%\" style=\"font-size: 0.8em;\"}\n- 13.5% (40.1 million) in 2023^[[@baumblatt_prevalence_2024]]\n\n- $412.9 billion in expenditures in 2022^[[@parker_economic_2024]]\n\n- Increasing complications: kidney failure, stroke, heart failure\n:::\n\n::: aside\n:::\n\n:::{.notes}\nLikely not new information to many in this room\n\nPoints to hit:\n\n- Prevalence generally increasing since 2001\n\n- More diabetes = more costs/negative outcomes\n\nTransition: None\n:::\n\n## Continuous Glucose Monitors (CGM)\n\n::: {.absolute top=\"42%\" left=\"20%\" width=\"550px\" height=\"auto\"}\n\n::: {style=\"margin-bottom: 0px\"}\n![](Images/CGM_Image.jpg)\n:::\n\n::: {.r-stack style=\"margin-top: 0px; font-size: 0.5em;\"}\nExample CGM device and readout from [NIDDK](https://www.niddk.nih.gov/health-information/professionals/diabetes-discoveries-practice/clinical-targets-for-continuous-glucose-monitoring-data).\n:::\n\n:::\n\n:::{.absolute top=\"15%\" left=\"0%\"}\n- Estimates blood glucose every 1-15 minutes, $\\leq 15$ days\n\n- Recommended for diabetes management^[[@american_diabetes_association_professional_practice_committee_for_diabetes_1_2025]]\n\n- System response to food, exercise, etc.\n:::\n\n::: aside\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Introduced by medical device companies to help those with diabetes maintain control of their glucose\n\n- Small wearable devices that are placed on the back of the upper arm, filament inserted into the skin, measures glucose there, uses it to estimate blood glucose\n\n- Sampling rate depends on device, wear period dictated by infection risk\n\n- Recommended for management by ADA for any populations with diabetes, particularly those at risk of dangerous low blood glucose\n\n    - Type 1, Type 2 using glucose lowering medications\n    \n- From a research perspective, we can use these devices to observe glucose system response to stimuli, including lifestyle interventions\n\nTransition: None\n:::\n\n## DASH4D-CGM\n\n::: {.absolute top=\"16%\" left=\"0%\"}\n- Dietary Approaches to stop Hypertension for Diabetes\n:::\n\n:::{.fragment .absolute top=\"24%\" left=\"0%\"}\n- $N = 89$ adults with type 2 diabetes\n\n- Randomized, crossover feeding study^[[@pilla_dietary_2025]]\n\n    - Meals from study center\n    \n    - DASH4D vs. Comparison\n    \n    - Wore blinded CGM\n  \n:::\n\n:::{.fragment .absolute top=\"66%\" left=\"0%\"}\n- DASH4D reduces mean glucose, increases time in normal range^[[@fang_dash4d_2025]]\n\n- [**How does DASH4D affect postprandial glucose response (PPGR)?**]{style=\"color:blue;\"}\n:::\n\n\n::: {.absolute top=\"23.5%\" left=\"61%\" width=\"440px\" height=\"auto\"}\n\n::: {style=\"margin-bottom: 0px\"}\n![](Images/Diet.png)\n:::\n\n::: {.r-stack style=\"margin-top: 0px; font-size: 0.5em;\"}\nExample DASH4D-compliant meal from the  [study website](https://publichealth.jhu.edu/welch-center-for-prevention-epidemiology-and-clinical-research/dash4d-a-modified-dash-diet-for-people-with-diabetes).\n:::\n:::\n\n::: aside\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Brings us to the DASH4D study I have had the great pleasure of being a part of. \n\n- Dietary approaches to stop hypertension for diabetes, dash-style diet adapted to those with diabetes (adjusted micro/macronutrients to control glucose load). Goal was to assess impact of this diet on measures of glycemic control.\n\n- Study details, DON'T DO TOO MUCH\n\n- Based on the study data, we already know that DASH4D reduces mean CGM glucose and increases Time in normal range\n\n- We are interested instead in more granular postprandial glucose response (glucose after meals). This should be the main mechanism through which the diet works, but that has not been quantified nor shown.\n\nTransition: But how do we get postprandial data from CGM?\n:::\n\n## CGM Data\n\n\n::: {.cell}\n<style type=\"text/css\">\n.my_class1 {\n  font-size: 0.9em;\n  height: 120px;\n  width: 500px;\n  max-height: 120px !important;\n  max-width: 500px !important;\n}\n\n.my_class2 {\nheight: 100px;\nwidth: 200px;\n}\n\n/* Hide scrollbar for WebKit browsers (Chrome, Safari, Opera) */\n.sourceCode::-webkit-scrollbar {\n  display: none;\n}\n\n/* Hide scrollbar for IE, Edge, and Firefox */\n.sourceCode {\n  -ms-overflow-style: none;  /* IE and Edge */\n  scrollbar-width: none;  /* Firefox */\n}\n</style>\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n:::{.absolute left=\"5%\" top=\"13%\"}\n![](Images/cgm_iso1.png){height=\"620px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::{.fragment}\n\n:::{.absolute left=\"5%\" top=\"13%\"}\n![](Images/cgm_iso2.png){height=\"620px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::{.absolute left=\"2%\" top=\"23%\"}\n$65$ participants\n:::\n\n:::\n\n:::{.fragment}\n\n:::{.absolute left=\"5%\" top=\"13%\"}\n![](Images/cgm_iso3.png){height=\"620px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::{.absolute left=\"2%\" top=\"23%\"}\n$65$ participants\n:::\n\n:::{.absolute left=\"7%\" top=\"63%\"}\n$768$ PPGR curves\n\n- $3$-$15$ per subject\n:::\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n:::{.notes}\nPoints to hit:\n\n- Raw data: 3 days from a single participant\n\n- Close collaboration produced validated postprandial responses for 65 participants\n\n- Mark them with dashed vertical, extract data around those points\n\n    - 4 hours after: postprandial response\n    \n    - 1 hour before: any changes in fasting, or resting glucose\n\n- Can collect all such curves for each participant, highlighting those marked above (mention number)\n\nTransition: How does existing literature model this data?\n:::\n\n## Summarizing PPGR\n\n:::{.absolute left=\"0%\" top=\"20%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/traditional_ppgr_1-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute left=\"0%\" top=\"20%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/traditional_ppgr_2-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute left=\"20%\" top=\"82%\"}\n**Mixed effects model at each time $t$**\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Existing analyses pick a single time point (draw vertical line and collect intersections with the curves)\n\n- To see the structure of the data, we have marked the person-specific means for each diet in blue, contrasting with the overall pooled means for each diet in red. \n\n- Shows the hierarchical structure: subject means deviate from the population mean and raw observations are distributed around the subject means\n\n- Using just this data, we can directly applied mixed effects modeling\n\nTransition: None\n:::\n\n## Linear Mixed Models {.smaller}\n\n:::{.absolute top=\"15%\" left=\"5%\" style=\"font-size: 1.2em;\"}\n$$PPGR^{(60)}_{ij} = \\beta_0^{(60)} + DASH4D_{ij} \\times \\boxed{\\boldsymbol{\\beta_1^{(60)}}} + (\\ldots) + U_i^{(60)} + \\epsilon_{ij}^{(60)}$$\n:::\n\n:::{.absolute top=\"33%\" left=\"0%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/est_1hour-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- So we can fit those models - go through the model definition (don't mention superscripts until the end)\n\n- Covariates accounted for: age, sex, BMI, time of day\n\n- We plot the beta1 estimate here at 1 hour\n\nTransition: None\n:::\n\n## Linear Mixed Models {.smaller}\n\n:::{.absolute top=\"15%\" left=\"5%\" style=\"font-size: 1.2em;\"}\n$$PPGR^{(120)}_{ij} = \\beta_0^{(120)} + DASH4D_{ij} \\times \\boxed{\\boldsymbol{\\beta_1^{(120)}}} + (\\ldots) + U_i^{(120)} + \\epsilon_{ij}^{(120)}$$\n:::\n\n:::{.absolute top=\"33%\" left=\"0%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/est_2hour-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Can do the same thing at the 2 hour mark\n\n- Note change in superscript, indicates different set of data is used\n\n- Estimates are similar\n:::\n\n## Linear Mixed Models {.smaller}\n\n:::{.absolute top=\"15%\" left=\"5%\" style=\"font-size: 1.2em;\"}\n$$PPGR^{(t)}_{ij} = \\beta_0^{(t)} + DASH4D_{ij} \\times \\boxed{\\boldsymbol{\\beta_1^{(t)}}} + (\\ldots) + U_i^{(t)} + \\epsilon_{ij}^{(t)}$$\n:::\n\n:::{.absolute top=\"33%\" left=\"0%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/est_allhour-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Nothing stopping us from fitting at all time points independently\n\n- This treats the data as independent, which is not necessarily true\n\n- For those familiar with longitudinal and multilevel modeling, accounting for correlation time point to time point is necessary to have valid inferences on estimates\n\n    - Even if estimates themselves don't change much (OLS gives unbiased estimates when there is not much missing data)\n    \nTransition: None\n:::\n\n## Agenda\n\n:::{style=\"font-size: 1.3em;\"}\n- [Modeling Continuous Glucose Monitor (CGM) Data]{style=\"opacity: 0.4;\"}\n\n- A Bayesian Functional Model for CGM Data\n\n- [Model Performance]{style=\"opacity: 0.4;\"}\n\n- [Model Inference on CGM]{style=\"opacity: 0.4;\"}\n\n- [Extending our approach]{style=\"opacity: 0.4;\"}\n:::\n\n:::{.notes}\nTransition: This brings us to joint modeling of the data accounting for the correlation structure, where we adapt a functional perspective and Bayesian fitting framework\n:::\n\n## Moving to a Functional Model\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n:::{.absolute top=\"13%\" left=\"0%\" style=\"font-size: 0.9em;\"}\n[Sources of Structure:]{.underline}\n:::\n\n:::{.fragment}\n\n:::{.absolute top=\"13%\" left=\"35%\" style=\"font-size: 0.9em;\"}\n- **Temporality**\n:::\n\n:::{.absolute top=\"22%\" left=\"0%\"}\n![](Images/Smooths.gif){height=\"400px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::\n\n:::{.fragment}\n\n:::{.absolute top=\"13%\" left=\"35%\" style=\"font-size: 0.9em;\"}\n- **Temporality**\n:::\n\n:::{.absolute top=\"22%\" left=\"0%\"}\n![](Images/Orig_Est.png){height=\"400px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::\n\n:::{.fragment}\n\n:::{.absolute top=\"13%\" left=\"57%\" style=\"font-size: 0.9em;\"}\n- **Within subject**\n:::\n\n:::{.absolute top=\"22%\" left=\"0%\"}\n![](Images/Within_Subject.png){height=\"400px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::\n\n:::{.fragment}\n\n:::{.absolute top=\"77%\" left=\"10%\" style=\"font-size: 0.9em;\"}\n$$\\boxed{PPGR_{ij}(t) = \\beta_0(t) + DASH4D_{ij} \\times \\boldsymbol{\\beta_1(t)} + (\\ldots) + U_i(t) + \\epsilon_{ij}(t)}$$\n:::\n\n:::\n\n\n:::{.notes}\nPoints to hit:\n\n- CGM data are highly correlated time point to time point, see that by tracing the data curves\n\n    - Agrees with our understanding of the physiology\n\n- This correlation in the response should transfer to the associations between it and a fixed predictor (we see this in our point-wise estimates, actually)\n\n- Another source of correlation is clustering of curves within subject\n\n    - Postprandial glucose dynamics might depend upon the subject (particularly their diabetes status, etc)\n    \n- To account for both sources, we introduce a functional mixed effects model (detail it)\n\n    -  Interpretations of the coefficient functions are consistent with non-functional models: $\\beta_p(t)$: association with PPGR at time $t$\n\nTransition: As there are numerous methods for estimating temporal fixed effects (splines over time, etc.), we will set the fixed effects aside for now and turn to the random effect U_i(t).\n:::\n\n## Modeling $U_i(t)$\n\n:::: {.columns .absolute top=\"20%\" left=\"0%\" style=\"max-width: none;\"}\n\n::: {.column width=\"460px\"}\n[Longitudinal LME Approach]{.underline}\n<div style=\"margin-top: 40px;\"></div>\n$U_i(t) = b_{i0} + b_{i1} \\times t + \\ldots$\n<div style=\"margin-top: 40px;\"></div>\n- $\\{1, t, \\ldots \\}$: chosen functions\n\n- $\\{b_{i0}, b_{i1}, \\ldots \\}$: correlated RE\n\n    - $(b_{i0}, b_{i1}, \\ldots)^t \\sim MVN(\\mathbf{0}, \\Sigma)$\n:::\n\n::: {.column width=\"60px\"}\n:::\n\n::: {.column width=\"540px\"}\n\n:::{.fragment}\n\n[Functional PCA]{.underline}\n<div style=\"margin-top: 40px;\"></div>\n$U_i(t) = \\sum_{k = 1}^K \\xi_{ik} \\phi_k(t)$\n<div style=\"margin-top: 40px;\"></div>\n- $\\phi_k(t)$: data-driven functional PCs\n\n- $\\xi_{ik}$: scores \n\n    - $\\xi_{ik} \\sim N(0, \\lambda_k)$ independent\n:::\n\n:::\n\n::::\n\n:::{.notes}\nPoints to hit:\n\n- Standard approach: choose bases and let coefficients be flexible multivariate gaussian\n\n    - Might not represent the data well\n    \n    - Could become computationally challenging as the number of bases increases\n    \n- Instead, we use functional principal components analysis (functional PCA)\n\n    - Let the data decide the basis, FPCs are orthonormal modes of variability that capture the most variability in the data\n    \n    - FPCs <-> eigenvectors with smoothing/continuity assumption (including the ordering)\n    \nTransition: Using this representation of the subject-specific random functions, we can return to the functional model for postprandial glucose\n:::\n\n## A Functional Model for PPGR\n\n:::{.absolute top=\"7%\" left=\"0%\"}\n$$PPGR_{ij}(t) = \\beta_0(t) + DASH4D_{ij} \\times \\beta_1(t) + (\\ldots) + \\sum_{k = 1}^K \\xi_{ik} \\phi_k(t) + \\epsilon_{ij}(t)$$\n:::\n\n:::{.absolute top=\"27%\" left=\"-5%\"}\n[Standard approach]{.underline}^[[@scheipl_functional_2015; @crainiceanu_functional_2024]]:\n\n1. Estimate $\\beta_p(t)$\n\n2. Estimate $\\widehat{\\phi}_k(t)$ from residuals\n\n3. Condition on splines, $\\widehat{\\phi}_k(t)$\n\n4. Fit linear mixed effects model\n:::\n\n\n:::{.fragment .absolute top=\"27%\" left=\"53%\"}\n**What about uncertainty in $\\widehat{\\phi}_k(t)$?**\n:::\n\n:::{.fragment .absolute top=\"37%\" left=\"53%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fpca_1-1.png){width=576}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"37%\" left=\"53%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fpca_2-1.png){width=576}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"37%\" left=\"53%\"}\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fpca_3-1.png){width=576}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"74%\" left=\"-5%\"}\n1. **[Account for $\\widehat{\\phi}_k(t)$ uncertainty?]{style=\"color:blue;\"}**\n\n2. **[Changed $\\beta_p(t)$ inference?]{style=\"color:blue;\"}**\n:::\n\n:::{.fragment .absolute top=\"83%\" left=\"53%\"}\n3. **[Subject-level inference?]{style=\"color:blue;\"}**\n:::\n\n::: aside\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Same functional model with $U_i(t)$ replaced by the FPCA decomposition\n\n- Go through standard procedure\n\n    - Recall can get good FE estimates (just not inference) in the presence of correlation\n    \n    - Fix spline basis for FE, FPC basis for RE, becomes a simple mixed effects model in the coefficients\n    \n- What if there is uncertainty in the FPC estimates?\n\n    - Show for our data using resampling, see some rather large deviations\n    \n- Brings us to our methodological questions\n\n    - Will cover 1 and 2 immediately, promise to get to 3 shortly.\n    \nTransition: None\n:::\n\n## Modeling $\\phi_k(t)$ as Parameters\n\n:::{.absolute top=\"16%\" left=\"-5%\"}\n[**Challenges modeling FPCs:**]{.underline}\n\n:::{style=\"font-size: 0.8em\"}\n- Constrained to be orthogonal (Stiefel manifold)\n    \n- Maintain smoothness\n:::\n\n:::\n\n:::{.fragment .absolute top=\"41%\" left=\"-5%\"}\n[**Literature:**]{.underline}\n\n:::{style=\"font-size: 0.68em;\"}\n- \"A Geometric Approach to Maximum Likelihood Estimation of the Functional Principal Components From Sparse Longitudinal Data\" [@peng_geometric_2009]\n\n- \"Generalized Multilevel Function-on-Scalar Regression and Principal Component Analysis\" [@goldsmith_generalized_2015]\n\n- \"Monte Carlo Simulation on the Stiefel Manifold via Polar Expansion\" [@jauch_monte_2021]\n\n- \"Functional principal component models for sparse and irregularly spaced data by Bayesian inference\" [@ye_functional_2023]\n\n- \"Bayesian Functional Principal Components Analysis via Variational Message Passing with Multilevel Extensions\" [@nolan_bayesian_2023]\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- FPCs exist on non-euclidean Stiefel manifold (space of all orthogonal matrices of particular dimension)\n\n- Further, they occupy a sub-space of this manifold over which certain continuity/smoothness criteria are met\n\n- Lots of approaches, from numerical methods on the manifold to fully Bayesian and variational modeling approaches, but there still is not a flexible, computationally efficient, fully-Bayesian approach to this modeling problem.\n\nTransition: This is the gap we seek to address with our MCMC Approach. Many have reservations about the computational efficiency of MCMC, but I assure you we have taken many steps to assure efficiency. With that said, lets get into the details\n:::\n\n## The FAST Approach^[[@Sartini08122025]]\n\n:::{.absolute top=\"15%\" style=\"font-size: 0.9em;\"}\n1. $\\phi_k(t) = \\mathbf{B}(t)\\psi_k$ for orthonormal splines $\\mathbf{B}(t) = \\{B_1(t), \\ldots , B_Q(t)\\}$\n:::\n\n:::{.absolute top=\"23%\" style=\"font-size: 0.9em;\"}\n- $\\phi_k(t)$ are orthonormal $\\iff$ vectors $\\psi_k$ are orthonormal\n:::\n    \n:::{.fragment .absolute top=\"31%\" style=\"font-size: 0.9em;\"}    \n- Substantial dimension reduction\n\n    - FPC Functions: $\\text{dim}(\\phi_k(t)) = \\infty$\n    \n    - FPC Vectors: $\\text{dim}(\\phi_k(t)) >> 100$\n    \n    - FPC Spline Coefficients: $\\text{dim}(\\psi_k) \\in [20, 50]$\n:::\n\n:::{.fragment .absolute top=\"62%\" style=\"font-size: 0.9em;\"}\n- Choice of basis $\\mathbf{B}(t)$ is **crucial**\n\n    - Restrict to well-behaved $\\phi_k(t)$\n    \n    - Shapes the $\\psi_k$ space\n:::\n\n::: aside\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Most important point: expand FPCs using an orthonormal basis (FPCs are orthonormal iff the spline coefficients are - have not solved the problem of sampling under orthonormality)\n\n- Basis projection inherently relies upon the functional structure of the data - smooth functions in time are often well-represented by a small number of splines (literature says 20-50 in many applications). Because of this, we achieve substantial dimension reduction\n\n- Two ways to look at $\\phi_k(t)$, as a function (where we are unable to do any types of calculation) and as a large vector (where computation becomes prohibitively expensive). We instead deal with a moderately sized set of parameters. \n\n- Choice of the basis further restricts the types of functions we are interested in, and shapes the space inhabited by the coefficients. We find a flexible, localized basis of orthonormal splines to be best. \n\nTransition: None\n:::\n\n## The FAST Approach Cont.\n\n:::{.absolute top=\"20%\" left=\"55%\"}\n\n::: {.cell}\n\n:::\n\n![](Images/Smoothness.png){height=\"180px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::{.absolute top=\"15%\" left=\"-5%\"}\n2. Penalized spline priors^[[@cravenwahba1979]] controlling \"wiggliness\"\n\n    - Add $-h_k \\int [\\phi_k''(t)]^2 dt$ to log-likelihood \n    \n    - Unique smoothing parameters $h_k$\n:::\n\n:::{.fragment}\n\n:::{.absolute top=\"47%\" left=\"-5%\"}\n3. Model $\\Psi = [\\psi_1 | \\ldots | \\psi_K]$ using parameter expansion^[[@jauch_monte_2021]]\n\n    - Sample unconstrained $\\mathbf{X}, X_{i,j} \\sim N(0, 1)$\n    \n    - Take SVD $\\mathbf{X} = \\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^t$\n    \n    - $\\mathbf{UV}^t$ is uniform on Stiefel manifold^[[@chikuse_statistics_2003]]\n:::\n\n:::{.absolute top=\"58%\" left=\"58%\"}\n\n::: {.cell .my_class1}\n\n```{.r .my_class1 .cell-code}\nX = matrix(rnorm(100), \n           ncol = 10, nrow = 10)\nSVD = svd(X)\nPsi = SVD$u %*% t(SVD$v)\n```\n:::\n\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Introduce smoothing prior (describe). \n\n    - Separate smoothing parameters (and stochastic) important here. \n    \n    - Just talked about dimension reduction, but we want to restrain the space even further to stabilize the MCMC sampler - so we introduce a smoothing prior\n\n- With the third point, we finally address sampling spline coefficients under orthonormality constraints. \n\n    - Use very standard parameter expansion approach - easier to model an unconstrained parameter and transform to the constrained space\n    \n    - Our approach follows that of Jauche and Dunson, leveraging some older statistical results on the Stiefel manifold (Chikuse). \n    \n    - Discuss the ease of computation due to simplicity of SVD, small size of coefficient matrix\n\n- Sampling the latent X, transforming, then applying the posterior smoothing penalty allows us to respect orthonormality constraints while enforcing smoothing of the FPCs\n\nTransition: Combining orthonormality and smoothing did not provide a known distirbutional form, so we had to make sure that this prior was proper for our model to be well-formulated.\n:::\n\n## FAST FPC Prior\n\n- $\\int [\\phi_k''(t)]^2 dt \\approx \\psi_k^t \\mathbf{P} \\psi_k$ for penalty matrix $\\mathbf{P}$ defined by $\\mathbf{B}(t)$\n\n<!-- with rank $\\mathbf{R}(\\mathbf{P})$ -->\n\n<!-- $$f(\\psi_k|h_k) \\propto h_k^{\\mathbf{R}(\\mathbf{P})/2}\\exp\\left(\\frac{-h_k\\psi_k^t \\mathbf{P} \\psi_k}{2} \\right) \\times \\mathbf{I}(\\psi_k \\text{ are orthonormal})$$ -->\n\n$$f(\\psi_k|h_k) \\propto \\text{MVN}\\left(\\mathbf{0}, (h_k\\mathbf{P})^{-1}\\right) \\times \\mathbf{I}(\\psi_k \\text{ are orthonormal})$$\n\n- $h_k$ smoothing parameters have Gamma$(\\alpha, \\beta)$ priors\n\n- Smoothing spline prior^[[@cravenwahba1979]] with additional orthonormality constraint\n\n[Proposition:]{.underline} The joint prior distribution on $\\psi_k, h_k$ is proper if and only if $2\\beta$ is greater than the first eigenvalue of the penalty matrix $\\mathbf{P}$\n\n::: aside\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Integral quantifying \"wiggliness\" can be approximated as a quadratic form in the spline coefficients with penalty matrix P which follows directly from the basis choice B\n\n- Spline coefficient prior given the corresponding smoothing parameter is proportional to MVN, but it is not a proper MVN due to the precision being singular\n\n- Smoothing parameters have independent gamma priors with chosen shapes alpha and rates beta\n\n- This is equivalent to a smoothing spline prior with the additional constraint of orthonormality\n\n- We can ensure that this condition holds in our model design. Set hyper-parameter beta based on the fixed penalty matrix (given the chosen basis).\n\nTransition: None\n:::\n\n## Agenda\n\n:::{style=\"font-size: 1.3em;\"}\n- [Modeling Continuous Glucose Monitor (CGM) Data]{style=\"opacity: 0.4;\"}\n\n- [A Bayesian Functional Model for CGM Data]{style=\"opacity: 0.4;\"}\n\n- Model Performance\n\n- [Model Inference on CGM]{style=\"opacity: 0.4;\"}\n\n- [Extending our approach]{style=\"opacity: 0.4;\"}\n:::\n\n:::{.notes}\nTransition: Ok, but how well does this combination of ideas work?\n:::\n\n## Simulation Validation \n\n:::{.absolute top=\"20%\" style=\"font-size: 1.4em;\"}\n- Real data and canonical examples\n\n- Similar/improved accuracy of point estimates\n\n- Nominal coverage of credible intervals\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Simulation scenarios mimicking real data and canonical examples\n\n- Similar/improved accuracy compared to fully-Bayesian and variational alternatives\n\n    - Accuracy improvement is more noticeable in the later FPCs\n    \n- Nominal coverage of posterior credible intervals\n\n- Closer look in the appendix\n\nTransition: Seems to do well, but what about the computational efficiency?\n:::\n\n## Computation\n\n:::{.absolute top=\"15%\" left=\"-2%\"}\n- $50-90\\%$ reduction relative to alternatives\n:::\n\n:::{.fragment .absolute top=\"23%\" left=\"-2%\"}\n- Our application: $768$ functions with $20$ obs: $\\approx 15$ minutes\n:::\n\n:::{.fragment .absolute top=\"31%\" left=\"-2%\"}\n- Simulations\n\n    - $50$ functions with $500$ obs: $\\approx 35$ minutes\n    \n    - $500$ functions with $50$ obs: $\\approx 15$ minutes\n    \n- Scales linearly in functions and obs.\n:::\n\n:::{.fragment}\n\n:::{.absolute top=\"63%\" left=\"-2%\"}\n- Efficiency improvements underway $\\rightarrow$\n:::\n\n:::{.absolute top=\"34%\" left=\"61%\"}\n\n::: {.cell}\n\n:::\n\n![](Images/Prelim_Compute.png){height=\"420px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Much faster than fully-Bayesian alternatives\n\n- Computations for moderate-scale problems are on the scale of minutes\n\n- Complexity currently scales linearly in terms of number of functions and number of observations per function\n\n- Actively working to improve this\n\n    - Experimental version does not scale with number of observations at all, making the approach feasible for modern, very dense data\n\nTransition: None\n:::\n\n## Agenda\n\n:::{style=\"font-size: 1.3em;\"}\n- [Modeling Continuous Glucose Monitor (CGM) Data]{style=\"opacity: 0.4;\"}\n\n- [A Bayesian Functional Model for CGM Data]{style=\"opacity: 0.4;\"}\n\n- [Model Performance]{style=\"opacity: 0.4;\"}\n\n- Model Inference on CGM\n\n- [Extending our approach]{style=\"opacity: 0.4;\"}\n:::\n\n:::{.notes}\nTransition: With that said, now we return to the problem that actually motivated all of this work, how does the DASH4D diet impact postprandial glucose response\n:::\n\n## DASH4D Diet Effect\n\n\n::: {.cell}\n\n:::\n\n\n:::{.absolute top=\"18%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/dash_FE-1.png){width=1152}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"18%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/dash_FE_highlight-1.png){width=1152}\n:::\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- See quite similar point estimates (as we would expect based upon multilevel modeling results)\n\n- Substantively: max drop of ~ 16 mg/dL between 1 and 2 hours, ~5 mg/dL fasting difference\n\n- Do see some small differences in widths of the uncertainty bounds\n\n    - Particularly in the fasting period where point-wise models indicate significance at this alpha level and FAST does not\n\nTransition: But we can learn even more from this data by virtue of choosing a Bayesian framework. We have posterior distributions for all model parameters of interest.\n:::\n\n## Variability Decomposition\n\n:::{.absolute top=\"12%\"}\n$$PPGR_{ij}(t) = \\color{#619CFF}{\\beta_0(t) + DASH4D_{ij} \\times \\beta_1(t) + (\\ldots)} + \\color{#00BA38}{\\sum_{k = 1}^K \\xi_{ik} \\phi_k(t)} + \\color{#F8766D}{\\epsilon_{ij}(t)}$$\n:::\n\n:::{.absolute left=\"-10%\" top=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/var_decomp-1.png){width=1344}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute left=\"-10%\" top=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/var_decomp_highlight-1.png){width=1344}\n:::\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- The fixed effects we were so interested in explain relatively little variability in the data\n\n    - We would be missing quite a bit if we performed prediction using just these elements\n    \n- Most variability explained at the subject level\n\nTransition: We can further understand what this variability looks like, a feature of using FPCA\n:::\n\n## Variability Decomposition\n\n:::{.absolute top=\"12%\"}\n$$PPGR_{ij}(t) = \\beta_0(t) + DASH4D_{ij} \\times \\beta_1(t) + (\\ldots) + \\sum_{k = 1}^K \\xi_{ik} \\phi_k(t) + \\epsilon_{ij}(t)$$\n:::\n\n:::{.absolute left=\"-12%\" top=\"37%\" height=\"400px\" width=\"640px\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/scree_plot-1.png){width=768}\n:::\n:::\n\n:::\n\n:::{.fragment}\n\n:::{.absolute left=\"50%\" top=\"37%\" height=\"400px\" width=\"640px\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/first_EF1-1.png){width=768}\n:::\n:::\n\n:::\n\n:::{.absolute left=\"10%\" top=\"39%\"}\n![](Images/Right_Long.png){width=\"480px\" height=\"auto\"}\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Almost all of the variability (according to eigenvalue) at the subject level is explained by the first FPC\n\n- First FPC looks a bit like a sterotypical CGM response (explain)\n\nTransition: So, what does this mean?\n:::\n\n## Variability Decomposition\n\n:::{.absolute top=\"12%\"}\n$$PPGR_{ij}(t) = \\beta_0(t) + DASH4D_{ij} \\times \\beta_1(t) + (\\ldots) + \\sum_{k = 1}^K \\xi_{ik} \\phi_k(t) + \\epsilon_{ij}(t)$$\n:::\n\n:::{.absolute left=\"-12%\" top=\"37%\" height=\"400px\" width=\"640px\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/first_EF2-1.png){width=768}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute left=\"50%\" top=\"37%\" height=\"432px\" width=\"640px\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/scores_ex-1.png){width=768}\n:::\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Participants with higher scores on the first FPC ($\\xi_{i1}$) will have trajectories which start higher and have more exaggerated response\n\n    - See for example the red points, chosen from a subject with high value for $\\xi_{i1}$\n    \n- In the reverse, those with lower scores $\\xi_{i1}$ will have lower starts and more mild response\n\n    - See for example the blue points\n    \nTransition: These insights were only possible because of the combination of a functional modeling perspective with a Bayesian framework.\n:::\n\n## Why FAST for Functional Modeling?\n\n:::{.absolute top=\"14%\"}\n- Fully-Bayesian: **estimates $+$ uncertainty for any quantity of interest**\n\n- Accounts for all known sources of correlation and uncertainty\n\n    - Valid inferences\n:::\n\n:::{.fragment .absolute top=\"39%\"}\n- Computationally efficient and stable\n\n- Implemented in standard software (STAN)\n:::\n\n:::{.fragment .absolute top=\"55%\"}\n- **We have developed an accessible R package**\n\n    - Available at <https://github.com/JSartini/BFun>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndash_fit = bfmm(Glucose ~ DASH4D + Age + BMI + Gender + TOD, id = ID)\n```\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Fully Bayesian - we get quite a lot for free with each run, and all known sources of correlation and uncertainty are accounted for (produces valid inference)\n\n- Efficient due to our choice of a restrictive prior/dimensionality reduction \n\n- Easily accessible - can implement or extend if you are aware of STAN\n\n- For those less familiar with STAN, can just use the R package we very recently made available on GitHub\n\n    - Still under active development (adding features and fixing bugs)\n\n    - Provides all necessary tools, including convergence monitoring\n\nTransition: Handy tool here, but can it do more? No assumptions were made which limit us from being able to apply it to a broad range of additional contexts\n:::\n\n## Generalization\n\n:::{.absolute top=\"14%\"}\n![](Images/Extensions.png)\n:::\n\n:::{.fragment .absolute top=\"14%\"}\n![](Images/Extensions_MSFAST.png)\n:::\n\n:::{.notes}\nPoints to hit:\n\n- I realized that there were many potential new contexts to which FAST could be adapted (detail them)\n\n- Around the time that I realized this, I started working with some irregularly-measured longitudinal growth data, leading to a particular extension being the first I explored\n\nTransition: None\n:::\n\n## Agenda\n\n:::{style=\"font-size: 1.3em;\"}\n- [Modeling Continuous Glucose Monitor (CGM) Data]{style=\"opacity: 0.4;\"}\n\n- [A Bayesian Functional Model for CGM Data]{style=\"opacity: 0.4;\"}\n\n- [Model Performance]{style=\"opacity: 0.4;\"}\n\n- [Model Inference on CGM]{style=\"opacity: 0.4;\"}\n\n- Extension to Multivariate, Sparse Data\n:::\n\n:::{.notes}\nTransition: Leads us into the final point, extending the model to a new scenario, specifically focusing on multivariate, sparse data collected as part of a child growth study.\n:::\n\n## The CONTENT Study^[[@checkley_effects_1998; @checkley_effects_2003]] \n\n::: {.absolute top=\"15%\" left=\"0%\"} \n- Helicopactor pylori $\\Rightarrow$ child growth\n\n- May 2007 - Feb 2011 near Lima City, Peru\n\n- Longitudinal cohort of $N = 197$ selected randomly from census\n:::\n\n:::{.fragment .absolute top=\"40%\" left=\"0%\"}\n- Length and weight measures^[[@crainiceanu_functional_2024]]\n\n    - Z-scored to age/gender WHO standards\n\n    - Increasing sparsity of observation over time\n    \n    - Missing/cancelled visits\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Basic information about the study (read slide here)\n\n- Z scores means that values can decrease (not an absolute negative change in length or weight)\n\n- Note that this data is packaged with the refund package in R.\n:::\n\n## CONTENT Data\n\n:::{.absolute top=\"12%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/content_obs-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"85%\" left=\"15%\"}\n**[Can we (dynamically) infer growth trajectories?]{style=\"color:blue;\"}**\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Data's increasing sparsity\n\n- Differential observation across participants\n\n- Correlation between measures\n\n- Can we predict trajectories for individuals with less data using information we learned from the others (leveraging the correlation between these two meaasures)?\n\nTransition: What do we mean by dynamic prediction\n:::\n\n## CONTENT Prediction\n\n:::{.absolute top=\"12%\" left=\"-3.27%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/just_obs_2-1.png){width=1028.16}\n:::\n:::\n\n:::\n\n:::{.absolute top=\"50%\" left=\"-5%\"}\n![](Images/CONTENT.gif){height=\"336px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Follow the points, shows what we would see as we collect the data\n\n- For this scenario and many similar ones, it is useful to predict where the measures are going given our understanding of their covariance and what we have seen for other participants\n\n    - Identify problematic trajectories early\n    \nTransition: Where does FAST come in for this problem? Well, we can use multivariate functional modeling to learn the covariance structure from our sparsely observed data.\n:::\n\n## Multivariate Functional PCA^[[@happ_multivariate_2018]]\n\n$$\\begin{pmatrix}W_{i}(t)\\\\ L_i(t) \\end{pmatrix}\n   = \\begin{pmatrix}\\mu^{(W)}(t; X_{i})\\\\ \\mu^{(L)}(t; X_{i}) \\end{pmatrix} + \\sum_{k = 1}^K \\xi_{ik} \\begin{pmatrix}\\phi^{(W)}_{k}(t)\\\\ \\phi^{(L)}_k(t) \\end{pmatrix} + \\begin{pmatrix}\\epsilon^{(W)}_i(t) \\\\ \\epsilon^{(L)}_i(t) \\end{pmatrix}$$\n\n:::{.absolute top=\"40%\" style=\"font-size: 0.92em;\"}\n- $\\mu^{(W)}(t; X_i), \\mu^{(L)}(t; X_i)$: variate-specific means\n\n- $\\xi_{ik} \\sim N(0, \\lambda_k)$: independent scores\n\n- $\\Phi_k(t) = \\begin{pmatrix} \\phi^{(W)}_k(t) \\\\ \\phi^{(L)}_k(t) \\end{pmatrix}$: joint, data-driven FPCs\n\n- $\\epsilon^{(W)}_i(t), \\epsilon^{(L)}_i(t)$: independent, normal errors\n:::\n\n:::{.fragment .absolute top=\"45%\" left=\"63%\"}\n\n::: {.cell}\n\n:::\n\n![](Images/Concatenate.png){height=\"80px\" width=\"auto\" style=\"border: 4px solid red;\"}\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Describe the equation, identical to univariate same statistics apply to concatenated functions\n\n- Point out the shared scores across variates, where the cross-covariances are held\n\n- Squint and it looks an awful lot like the univariate FPCA, very similar computationally if we stack and just smooth over individual domains\n\n- Show what I mean by concatenation\n\nTransition: So, FAST is applicable, but what changes to we need to make?\n:::\n\n## Multivariate, Sparse FAST (MSFAST)\n\n:::{.absolute top=\"20%\" left=\"-5%\"}\n[Adjustments:]{.underline}\n\n- Sparsity handled by splines\n\n- Concatenate the FPCs\n\n- Scale variates in pre-processing\n\n- More robust posterior FPC alignment\n:::\n\n:::{.fragment .absolute top=\"20%\" left=\"50%\"}\n[Simulations:]{.underline}\n\n- 2-4 variables\n\n- Compared to available software^[[@happ_multivariate_2018; @li_fast_2020; @nolan_efficient_2025]]\n\n    - $25$-$50\\%$ error reduction\n    \n    - Accurate estimates\n    \n    - Competitive computation\n\n- Nominal coverage\n:::\n\n::: aside\n:::\n\n:::{.notes}\nPoints to hit:\n\n- Each of the updates\n\n- Simulations validated\n\n    - Hit each of the given points\n    \n    - MSFAST is particularly better than alternatives for later eigenfunctions with smaller signals.\n:::\n\n## Trajectory Prediction\n\n:::{.absolute top=\"14%\" left=\"0%\" style=\"font-size: 0.75em;\"}\n1. Learn $\\phi_k^{(p)}(t)$, $\\mu^{(p)}(t; X_i)$\n\n2. New scores $\\xi_{ik}$ are conditionally Gaussian\n:::\n\n:::{.absolute top=\"30%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/pred1-1.png){width=1152}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"30%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/pred2-1.png){width=1152}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"30%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/pred3-1.png){width=1152}\n:::\n:::\n\n:::\n\n:::{.notes}\nPoints to hit:\n\n- How we do this: fit the model on the observed data, draw posterior of population parameters\n\n- Conditional distribution of new subject scores given the population parameters and the associated data is Gaussian, simple to draw from\n\n- Follow the female in the left: diverging is mitigated\n:::\n\n## Ongoing Work\n\n:::{.absolute top=\"14%\" left=\"4%\"}\n![](Images/Extensions_progress.png){height=\"560px\" width=\"auto\"}\n:::\n\n:::{.absolute top=\"83%\" left=\"70%\" style=\"font-size: 60%;\"}\n^1^[@Sartini08122025]\n\n^2^[@sartini_bayesian_2025]\n:::\n\n:::{.notes}\nPoints to hit:\n\n- What we have already covered\n\n- Dense data (touched on briefly before) makes FAST feasible for much higher-dimensional data\n\n- Generalized: CGM and ECG co-evolution (indicators for arrhythmia)\n\n    - General idea: follows the same principle as generalized linear models, greater detail/visualization if wanted\n\n- Nothing stopping us from doing repeat measures\n\nTransition: None\n:::\n\n## {#Review}\n\n:::{.absolute top=\"0%\"}\n![](Images/Loop.png){height=\"600px\" width=\"auto\"}\n:::\n\n:::{.fragment .fade-in-then-semi-out}\n\n:::{.absolute top=\"10%\" left=\"22.5%\"}\n![](Images/Highlight.png){height=\"100px\" width=\"auto\"}\n:::\n\n:::{.absolute top=\"0%\" left=\"8%\"}\n![](Images/JustSensor.png){height=\"200px\" width=\"auto\"}\n:::\n\n:::\n\n:::{.fragment .fade-in-then-semi-out}\n\n:::{.absolute top=\"10%\" left=\"54%\"}\n![](Images/Highlight.png){height=\"100px\" width=\"auto\"}\n:::\n\n:::{.absolute top=\"0%\" left=\"75%\"}\n![](Images/Welch_Collab.png){height=\"250px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::\n\n:::{.fragment .fade-in-then-semi-out}\n\n:::{.absolute top=\"58.5%\" left=\"54%\"}\n![](Images/Highlight.png){height=\"100px\" width=\"auto\"}\n:::\n\n:::{.absolute top=\"40%\" left=\"75%\"}\n![](Images/Biostats_Collab.png){height=\"125px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::{.absolute top=\"60%\" left=\"75%\"}\n![](Images/Data_Spline.gif){height=\"240px\" width=\"auto\" style=\"max-width: none;\"}\n:::\n\n:::\n\n:::{.fragment .fade-in-then-semi-out}\n\n:::{.absolute top=\"58.5%\" left=\"22.5%\"}\n![](Images/Highlight.png){height=\"100px\" width=\"auto\"}\n:::\n\n:::{.absolute top=\"80%\" left=\"5%\" style=\"font-size: 0.5em;\"}\n- FAST: <https://github.com/JSartini/FAST-BFPCA>\n\n- MSFAST: <https://github.com/JSartini/MSFAST_B-m-FPCA>\n\n- Package: <https://github.com/JSartini/BFun>\n:::\n\n:::\n\n:::{.fragment}\n\n:::{.absolute top=\"10%\" left=\"22.5%\"}\n![](Images/Highlight.png){height=\"100px\" width=\"auto\"}\n:::\n\n:::{.absolute top=\"28%\" left=\"-3%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/content_visuals-1.png){width=192}\n:::\n:::\n\n:::\n\n:::\n\n:::{.fragment .absolute top=\"24.5%\" left=\"15%\"}\n![](Images/ECG.png){height=\"160px\" width=\"auto\"}\n:::\n\n:::{.notes}\nJust a bit summary for this talk, as well as my research philosophy as a whole\n\nPoints to hit:\n\n- Start with a scientific problem I am interested in (can talk about Cooper)\n\n- Engaged collaboration with subject matter experts, name everyone\n\n- Based on what I learn from the data and my own statistical background, worked with Dr. Zeger and Crainiceanu to form a particular representation of the data and corresponding algorithm\n\n- Actually implement it and make it available \n\n- Can take it to new problems, for example the growth trajectory problem or new wearables (e-patch)\n\nThank you, I would be happy to answer any questions\n:::\n\n\n## References {.scrollable}\n\n::: {#refs .smallest}\n:::\n\n# Appendices \n\n## DASH4D Design\n\n:::{.absolute top=\"30%\" left=\"-10%\"}\n![](Images/FlowChart.png){width=\"1250px\" height=\"auto\" style=\"max-width: none;\"}\n:::\n\n## FAST Accuracy {visibility=\"uncounted\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fast_ISE-1.png){width=960}\n:::\n:::\n\n\n## FAST Coverage {visibility=\"uncounted\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fast_COV-1.png){width=960}\n:::\n:::\n\n\n## Fast Univariate Inference^[[@cui_fast_2022]] {visibility=\"uncounted\"}\n\n\n::: {.cell}\n\n:::\n\n\n:::{.absolute top=\"15%\"}\n1. Fit local linear mixed effects models\n\n2. Smooth along the functional domain\n\n3. Point-wise confidence bands using the smoothing operator\n\n4. Joint confidence bands using analytic procedure or bootstrap\n:::\n\n:::{.absolute top=\"52%\" left=\"5%\" height=\"300px\" width=\"800px\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fui_vis_1-1.png){width=768}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"52%\" left=\"5%\" height=\"300px\" width=\"800px\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fui_vis_2-1.png){width=768}\n:::\n:::\n\n:::\n\n\n:::{.notes}\nFUI is an alternative method (from our group) which can fit types of models. FUI is focused on just fixed effects estimation, so it does not model the random effects explicitly.\n\nBe sure to say:\n\n- Very computationally efficient (parallelizable)\n\n- Does not model the random effects $U_i(t)$, marginalizes them out\n:::\n\n::: aside\n:::\n\n## FAST vs FUI {visibility=\"uncounted\"}\n\n:::{.absolute top=\"18%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/vis_std_comp-1.png){width=1152}\n:::\n:::\n\n:::\n\n<!-- :::{.absolute top=\"74%\" left=\"65%\" style=\"font-size: 0.5em;\"} -->\n<!-- \"FUI: Fast Univariate Inference [@cui_fast_2022]\" -->\n<!-- ::: -->\n\n:::{.fragment}\n\n:::{.absolute top=\"18%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/vis_std_comp_zoom-1.png){width=1152}\n:::\n:::\n\n:::\n\n<!-- :::{.absolute top=\"74%\" left=\"65%\" style=\"font-size: 0.5em;\"} -->\n<!-- \"FUI: Fast Univariate Inference [@cui_fast_2022]\" -->\n<!-- ::: -->\n\n\n:::{.absolute top=\"83%\" style=\"color:blue;\"}\n**Moving the needle for fasting glucose?**\n:::\n\n:::\n\n:::{.notes}\nVery similar fixed effects, but a bit different on the inference (only small, but deserves further interpretation)\n:::\n\n## FAST Timing - N {visibility=\"uncounted\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fast_Time_N-1.png){width=960}\n:::\n:::\n\n\n## FAST Timing - M {visibility=\"uncounted\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/fast_Time_M-1.png){width=960}\n:::\n:::\n\n\n## FASTer Description {visibility=\"uncounted\"} \n\n:::{.absolute top=\"15%\" left=\"-4%\"}\n- Project $Y_i(t) = \\mathbf{B}(t) \\eta_i$, **only** model $\\eta_i$\n\n- Choose $\\mathbf{B}(t)$ to be vector orthonormal (matrix $\\mathbf{B}$)\n\n    - $\\eta_i = (\\mathbf{B}^T \\mathbf{B})^{-1}\\mathbf{B}^t Y_i = \\mathbf{B}^t Y_i$, still independent\n    \n- **Computations should not scale with observations per function**\n:::\n\n:::{.fragment .absolute top=\"49%\" left=\"-4%\"}\n[Preliminary Results:]{.underline}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/computation_time-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::{.notes}\nFrom preliminary results - estimates and coverage match FAST. Computation times in box plot\n:::\n\n## FASTer Accuracy {visibility=\"uncounted\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/faster_ISE-1.png){width=960}\n:::\n:::\n\n\n## FASTer Coverage {visibility=\"uncounted\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/faster_cov-1.png){width=960}\n:::\n:::\n\n\n## MSFAST Trajectory Accuracy {visibility=\"uncounted\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/msfast_traj_ISE-1.png){width=960}\n:::\n:::\n\n\n\n## MSFAST Trajectory Coverage {visibility=\"uncounted\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/msfast_traj_cov-1.png){width=960}\n:::\n:::\n\n\n## MSFAST FPC Accuracy {visibility=\"uncounted\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/msfast_fpc_ISE-1.png){width=960}\n:::\n:::\n\n\n## MSFAST FPC Coverage {visibility=\"uncounted\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/msfast_fpc_cov-1.png){width=960}\n:::\n:::\n\n\n## Generalized Functional PCA {visibility=\"uncounted\"}\n\n\n::: {.cell}\n\n:::\n\n\n:::{.absolute top=\"15%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/gen_data-1.png){width=1152}\n:::\n:::\n\n:::\n\n:::{.fragment .absolute top=\"15%\" left=\"-5%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](Job_Talk_files/figure-revealjs/gen_data_vis-1.png){width=1152}\n:::\n:::\n\n\n$$g\\bigl(\\mathbb{E}[Y_i(t)]\\bigr) = \\mu(t; X_{i}) + \\sum_{k = 1}^K \\xi_{ik}\\phi_k(t)$$\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}