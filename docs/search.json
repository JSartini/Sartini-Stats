[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n9/3/25\n\n\nMSFAST: Bayesian Multivariate, Sparse FPCA Vignette\n\n\nJoseph Sartini\n\n\n\n\n8/12/25\n\n\nVisualizing Climate Exposures using Gifs\n\n\nJoseph Sartini\n\n\n\n\n1/31/25\n\n\nSTAN Introduction\n\n\nJoseph Sartini\n\n\n\n\n11/24/24\n\n\nFAST: Bayesian FPCA Vignette\n\n\nJoseph Sartini\n\n\n\n\n11/22/24\n\n\nData Visualization Principles\n\n\nJoseph Sartini\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Glucose Color Index: Development and Validation of a Novel Measure of the Shape of Glycemic Variability\n    Sartini, Fang, Rooney, Selvin, Coresh, & Zeger\n    Diabetes Science and Technology\n    (2024)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    The evolution of private reputations in information-abundant landscapes\n    Michel-Mata, Kawakatsu, Sartini, Kessinger, Plotkin, & Tarnita\n    Nature\n    (2024)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    The Associations of Mean Glucose and Time in Range from Continuous Glucose Monitoring with HbA1c in Adults with Type 2 Diabetes\n    Selvin, Wang, Rooney, Echouffo-Tcheugui, Fang, Zeger, Sartini, Tang, Coresh, Aurora, & Punjabi\n    Diabetes Technology and Therapeutics\n    (2023)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n  \n\n  \n    Within-person and between-sensor variability in continuous glucose monitoring metrics\n    Selvin, Wang, Rooney, Fang, Echouffo-Tcheugui, Zeger, Sartini, Tang, Coresh, Aurora, & Punjabi\n    Clinical Chemistry\n    (2023)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pubs.html#in-print",
    "href": "pubs.html#in-print",
    "title": "Publications",
    "section": "",
    "text": "Glucose Color Index: Development and Validation of a Novel Measure of the Shape of Glycemic Variability\n    Sartini, Fang, Rooney, Selvin, Coresh, & Zeger\n    Diabetes Science and Technology\n    (2024)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    The evolution of private reputations in information-abundant landscapes\n    Michel-Mata, Kawakatsu, Sartini, Kessinger, Plotkin, & Tarnita\n    Nature\n    (2024)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    The Associations of Mean Glucose and Time in Range from Continuous Glucose Monitoring with HbA1c in Adults with Type 2 Diabetes\n    Selvin, Wang, Rooney, Echouffo-Tcheugui, Fang, Zeger, Sartini, Tang, Coresh, Aurora, & Punjabi\n    Diabetes Technology and Therapeutics\n    (2023)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n  \n\n  \n    Within-person and between-sensor variability in continuous glucose monitoring metrics\n    Selvin, Wang, Rooney, Fang, Echouffo-Tcheugui, Zeger, Sartini, Tang, Coresh, Aurora, & Punjabi\n    Clinical Chemistry\n    (2023)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pubs.html#pre-print",
    "href": "pubs.html#pre-print",
    "title": "Publications",
    "section": "Pre-Print",
    "text": "Pre-Print\n\n\n\n  \n    Bayesian Multivariate Sparse Functional PCA\n    Sartini, Zeger, & Crainiceanu\n    ArXiv\n    (2025)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n       Preprint\n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    Fast Bayesian Functional Principal Components Analysis\n    Sartini, Zhou, Selvin, Zeger, & Crainiceanu\n    ArXiv\n    (2025)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n       Preprint\n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    Prediction Inference Using Generalized Functional Mixed Effects Models\n    Zhou, Cui, Sartini, & Crainiceanu\n    ArXiv\n    (2025)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n       Preprint\n    \n    \n    \n  \n\n  \n    Multilevel functional data analysis modeling of human glucose response to meal intake\n    Matabuena, Sartini, & Gude\n    ArXiv\n    (2024)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n       Preprint\n    \n    \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joe Sartini",
    "section": "",
    "text": "I am a 5th year PhD student in the Biostatistics Department at the Johns Hopkins University Bloomberg School of Public Health.\nI am concluding my degree requirements, with tentative graduation in Spring 2026. My PhD advisors are Dr. Scott Zeger and Dr. Ciprian Crainiceanu. For my applied work, I am mentored by Dr. Elizabeth Selvin and Dr. Michael Fang. I previously completed a BSE in Operations Research and Financial Engineering from Princeton University in 2021.\n \n    \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n  \n      Bluesky\n  \n\n\n  \n\n\n\n\n\nMotivated by the increasing complexity of data collected by studies employing new physiological monitoring technology (e.g. wearable devices like accelerometry, continuous glucose monitoring, or patch electrocardiograms), my research combines a rigorous statistical approach with computational techniques to produce practical software for performing valid inference on structured longitudinal and functional data.\nWhen not working on methods or digging into a dataset, I can usually be found either weightlifting or running.\n\n\n\nStruggling post-Baltimore Marathon"
  },
  {
    "objectID": "resources/publications/in_print/selvin_variability_2023.html",
    "href": "resources/publications/in_print/selvin_variability_2023.html",
    "title": "Within-person and between-sensor variability in continuous glucose monitoring metrics",
    "section": "",
    "text": "Selvin, E., Wang, D., Rooney, M. R., Fang, M., Echouffo-Tcheugui, J. B., Zeger, S., Sartini, J., Tang, O., Coresh, J., Nisha Aurora, R., & Punjabi, N. M. (2023). Within-Person and Between-Sensor Variability in Continuous Glucose Monitoring Metrics. Clinical chemistry, 69(2), 180–188. https://doi.org/10.1093/clinchem/hvac192"
  },
  {
    "objectID": "resources/publications/in_print/selvin_variability_2023.html#citation-apa-7",
    "href": "resources/publications/in_print/selvin_variability_2023.html#citation-apa-7",
    "title": "Within-person and between-sensor variability in continuous glucose monitoring metrics",
    "section": "",
    "text": "Selvin, E., Wang, D., Rooney, M. R., Fang, M., Echouffo-Tcheugui, J. B., Zeger, S., Sartini, J., Tang, O., Coresh, J., Nisha Aurora, R., & Punjabi, N. M. (2023). Within-Person and Between-Sensor Variability in Continuous Glucose Monitoring Metrics. Clinical chemistry, 69(2), 180–188. https://doi.org/10.1093/clinchem/hvac192"
  },
  {
    "objectID": "resources/publications/in_print/selvin_variability_2023.html#abstract",
    "href": "resources/publications/in_print/selvin_variability_2023.html#abstract",
    "title": "Within-person and between-sensor variability in continuous glucose monitoring metrics",
    "section": "Abstract",
    "text": "Abstract\nBackground: The within-person and between-sensor variability of metrics from different interstitial continuous glucose monitoring (CGM) sensors in adults with type 2 diabetes not taking insulin is unclear. Methods: Secondary analysis of data from 172 participants from the HYPNOS randomized clinical trial. Participants simultaneously wore Dexcom G4 and Abbott Libre Pro CGM sensors for up to 2-weeks at baseline and, again, at the 3-month follow-up visit. Results: At baseline (up to 2-weeks of CGM), mean glucose for both the Abbot and Dexcom sensors was approximately 150 mg/dl and time-in-range (70-180 mg/dL) was just below 80%. When comparing the same sensor at two different time points (two 2-week wear periods, 3 months apart), the within-person variability (CVw) in mean glucose was 17.4% (Abbott) and 14.2% (Dexcom). CVw for percent time-in-range: 20.1% (Abbott) and 18.6% (Dexcom). At baseline, the Pearson’s correlation of mean glucose from the two sensors worn simultaneously was r=0.86, root mean squared error (RMSE), 13 mg/dL; for time-in-range, r=0.88, RMSE 8%-points. Conclusions: Substantial variation was observed within sensors over time and across two different sensors worn simultaneously on the same individuals. Clinicians should be aware of this variability when using CGM technology to make clinical decisions."
  },
  {
    "objectID": "resources/publications/in_print/sartini_GCI_2024.html",
    "href": "resources/publications/in_print/sartini_GCI_2024.html",
    "title": "Glucose Color Index: Development and Validation of a Novel Measure of the Shape of Glycemic Variability",
    "section": "",
    "text": "Sartini J, Fang M, Rooney MR, Selvin E, Coresh J, Zeger S. Glucose Color Index: Development and Validation of a Novel Measure of the Shape of Glycemic Variability. Journal of Diabetes Science and Technology. 2024;0(0). doi:10.1177/19322968241245654"
  },
  {
    "objectID": "resources/publications/in_print/sartini_GCI_2024.html#citation-apa-7",
    "href": "resources/publications/in_print/sartini_GCI_2024.html#citation-apa-7",
    "title": "Glucose Color Index: Development and Validation of a Novel Measure of the Shape of Glycemic Variability",
    "section": "",
    "text": "Sartini J, Fang M, Rooney MR, Selvin E, Coresh J, Zeger S. Glucose Color Index: Development and Validation of a Novel Measure of the Shape of Glycemic Variability. Journal of Diabetes Science and Technology. 2024;0(0). doi:10.1177/19322968241245654"
  },
  {
    "objectID": "resources/publications/in_print/sartini_GCI_2024.html#abstract",
    "href": "resources/publications/in_print/sartini_GCI_2024.html#abstract",
    "title": "Glucose Color Index: Development and Validation of a Novel Measure of the Shape of Glycemic Variability",
    "section": "Abstract",
    "text": "Abstract\nBackground: Standard continuous glucose monitoring (CGM) metrics: mean glucose, standard deviation, coefficient of variation, and time in range, fail to capture the shape of variability in the CGM time series. This information could facilitate improved diabetes management. Methods: We analyzed CGM data from 141 adults with type 2 diabetes in the Hyperglycemic Profiles in Obstructive Sleep Apnea (HYPNOS) trial. Participants in HYPNOS wore CGM sensors for up to two weeks at two time points, three months apart. We calculated the log-periodogram for each time period, summarizing using disjoint linear models. These summaries were combined into a single value, termed the Glucose Color Index (GCI), using canonical correlation analysis. We compared the between-wear correlation of GCI with those of standard CGM metrics and assessed associations between GCI and diabetes comorbidities in 398 older adults with type 2 diabetes from the Atherosclerosis Risk in Communities (ARIC) study. Results: The GCI achieved a test-retest correlation of R = .75. Adjusting for standard CGM metrics, the GCI test-retest correlation was R = .55. Glucose Color Index was significantly associated (p &lt; .05) with impaired physical functioning, frailty/pre-frailty, cardiovascular disease, chronic kidney disease, and dementia/mild cognitive impairment after adjustment for confounders. Conclusion: We developed and validated the GCI, a novel CGM metric that captures the shape of glucose variability using the periodogram signal decomposition. Glucose Color Index was reliable within participants over a three-month period and associated with diabetes comorbidities. The GCI suggests a promising avenue toward the development of CGM metrics which more fully incorporate time series information."
  },
  {
    "objectID": "resources/publications/pre_print/zhou_prediction_2025.html",
    "href": "resources/publications/pre_print/zhou_prediction_2025.html",
    "title": "Prediction Inference Using Generalized Functional Mixed Effects Models",
    "section": "",
    "text": "Citation (APA 7)\nZhou, X., Cui, E., Sartini, J., & Crainiceanu, C. (2025). Prediction Inference Using Generalized Functional Mixed Effects Models (arXiv:2501.07842). arXiv. https://doi.org/10.48550/arXiv.2501.07842\nAbstract\nWe introduce inferential methods for prediction based on functional random effects in generalized functional mixed effects models. This is similar to the inference for random effects in generalized linear mixed effects models (GLMMs), but for functional instead of scalar outcomes. The method combines: (1) local GLMMs to extract initial estimators of the functional random components on the linear predictor scale; (2) structural functional principal components analysis (SFPCA) for dimension reduction; and (3) global Bayesian multilevel model conditional on the eigenfunctions for inference on the functional random effects. Extensive simulations demonstrate excellent coverage properties of credible intervals for the functional random effects in a variety of scenarios and for different data sizes. To our knowledge, this is the first time such simulations are conducted and reported, likely because prediction inference was not viewed as a priority and existing methods are too slow to calculate coverage. Methods are implemented in a reproducible R package and demonstrated using the NHANES 2011-2014 accelerometry data."
  },
  {
    "objectID": "resources/publications/pre_print/sartini_MSFAST_2025.html",
    "href": "resources/publications/pre_print/sartini_MSFAST_2025.html",
    "title": "Bayesian Multivariate Sparse Functional PCA",
    "section": "",
    "text": "Citation (APA 7)\n\nSartini, J., Zeger, S., & Crainiceanu, C. (2025). Bayesian Multivariate Sparse Functional PCA (No. arXiv:2509.03512). arXiv. https://doi.org/10.48550/arXiv.2509.03512\n\nAbstract\nFunctional Principal Components Analysis (FPCA) provides a parsimonious, semi-parametric model for multivariate, sparsely-observed functional data. Frequentist FPCA approaches estimate principal components (PCs) from the data, then condition on these estimates in subsequent analyses. As an alternative, we propose a fully Bayesian inferential framework for multivariate, sparse functional data (MSFAST) which explicitly models the PCs and incorporates their uncertainty. MSFAST builds upon the FAST approach to FPCA for univariate, densely-observed functional data. Like FAST, MSFAST represents PCs using orthonormal splines, samples the orthonormal spline coefficients using parameter expansion, and enforces eigenvalue ordering during model fit. MSFAST extends FAST to multivariate, sparsely-observed data by (1) standardizing each functional covariate to mitigate poor posterior conditioning due to disparate scales; (2) using a better-suited orthogonal spline basis; (3) parallelizing likelihood calculations over covariates; (4) updating parameterizations and priors for computational stability; (5) using a Procrustes-based posterior alignment procedure; and (6) providing efficient prediction routines. We evaluated MSFAST alongside existing implementations using simulations. MSFAST produces uniquely valid inferences and accurate estimates, particularly for smaller signals. MSFAST is motivated by and applied to a study of child growth, with an accompanying vignette illustrating the implementation step-by-step."
  },
  {
    "objectID": "posts/Gifs.html",
    "href": "posts/Gifs.html",
    "title": "Visualizing Climate Exposures using Gifs",
    "section": "",
    "text": "library(GSODR)\nlibrary(tidyverse)\nlibrary(gifski)\nlibrary(gganimate)"
  },
  {
    "objectID": "posts/Gifs.html#introduction",
    "href": "posts/Gifs.html#introduction",
    "title": "Visualizing Climate Exposures using Gifs",
    "section": "Introduction",
    "text": "Introduction\nThe goal of this document is to present how to use some helpful animation tools within R (the gifski and gganimate packages) for the purposes of visualizing periodic or seasonal time series data as it varies over the course of a standard epoch. This is useful within a number of contexts, for example when working to understand climate-related exposures over the course of the year. These exposures have consistent seasonal patterns which repeat over the course of each year, so we can aggregate over years and subjects/locations to form estimates of expected exposure over time within any desired window. Animation then allows us to examine how this expected exposure differs as the year progresses. We procede with an example based on the Global Surface Summary of Day (GSOD) data through the corresponding API in the GSODR package."
  },
  {
    "objectID": "posts/Gifs.html#gsod-data",
    "href": "posts/Gifs.html#gsod-data",
    "title": "Visualizing Climate Exposures using Gifs",
    "section": "GSOD Data",
    "text": "GSOD Data\nThe GSOD API provides the “get_GSOD()” function which can be used to pull climate information collected by a subset of the GSOD stations. Here, I have selected Station 723400-99999, which is a Weather Service station in Little Rock, Arkansas, near where I grew up. We collect data between 1990-1995, focusing on humidity as indicated by the dew point (labelled “DEWP” in this dataset, measured in degrees Celsius). This choice of outcome was purely motivated by my own past experience enduring the humidity of my home state.\n\nLR_GSOD = get_GSOD(years = 1990:1995, station = \"723400-99999\") %&gt;%\n    mutate(wdate = ymd(YEARMODA)) %&gt;%\n    rename(dewpoint = DEWP) %&gt;%\n    select(wdate, dewpoint) %&gt;%\n    drop_na()"
  },
  {
    "objectID": "posts/Gifs.html#stationary-visualization-of-gsod-data",
    "href": "posts/Gifs.html#stationary-visualization-of-gsod-data",
    "title": "Visualizing Climate Exposures using Gifs",
    "section": "Stationary Visualization of GSOD Data",
    "text": "Stationary Visualization of GSOD Data\nWe can first create a simple visualization of this time series data, using a point plot with smoothing line to understand (seasonal) trends and day-to-day variability after adjusting for these trends.\n\nLR_GSOD %&gt;%\n  ggplot(aes(x = wdate, y = dewpoint)) + \n  geom_point(alpha = 0.4) +\n  geom_smooth(se = F, color = \"steelblue\", method = \"gam\", \n              formula = y ~ s(x, bs = \"cs\", k = 35)) + \n  theme_bw() + \n  labs(x = \"Date\", y = \"Dewpoint (°C)\")"
  },
  {
    "objectID": "posts/Gifs.html#example-animation",
    "href": "posts/Gifs.html#example-animation",
    "title": "Visualizing Climate Exposures using Gifs",
    "section": "Example Animation",
    "text": "Example Animation\nWhile the above visualization is sufficient for many applications, what about understanding how patterns of dew point over a fixed period, say 6 months, change as one moves through the year? This could be done by sliding a window over the above plot and juxtaposing the visualizations that result from different window locations. However, creating these windowed visualizations and finding the most interesting contrasts can be time consuming.\nAs an alternative, we can create an animation which demonstrates the evolution of this windowed exposure over the course of the year. Using the gifski and gganimate packages, generating this type of visualization is straightforward. We first expand the data, collecting the appropriate window for each possible start date.\n\nwindow_length = 6*4 # in weeks\n\ngif_df = LR_GSOD %&gt;%\n  group_split(wdate) %&gt;%\n  map(function(x){\n    sday = first(x$wdate)\n    \n    sub_data = LR_GSOD %&gt;%\n      mutate(dt = as.numeric(difftime(wdate, sday, units = \"weeks\"))) %&gt;%\n      filter(dt &gt;= 0 & dt &lt;= window_length)\n    \n    sub_data$month_start = month(sday)\n    sub_data$day_start = mday(sday)\n    \n    return(sub_data)\n  }) %&gt;% \n    list_rbind() %&gt;%\n    mutate(day_idx = ymd(\"1999-12-31\") + months(month_start) + days(day_start))\n\ngraph1 = gif_df %&gt;%\n  ggplot(aes(x = dt, y = dewpoint)) + \n  geom_point(alpha = 0.4, size = 3) +\n  geom_smooth(se = F, color = \"steelblue\") +\n  theme_bw() + \n  labs(x = \"Week of Exposure Window\", y = \"Dewpoint (°C)\")\n\ngraph1.animation = graph1 + \n  transition_time(day_idx) + \n  labs(subtitle = \"Start of Window: {format(frame_time, '%m-%d')}\")\n\nanimate(graph1.animation, height = 500, width = 900, duration = 20, \n        end_pause = 1, nframes = 200)\nanim_save(\"../resources/images/Gif_Exposure.gif\")\n\nWe can now view the saved animation through embedding:\n\n\n\nThe exposure animation generated in R.\n\n\nWhile this type of animation is helpful in and of itself, it is best combined with similar animations for related outcome variables and other covariates of interest. Dynamic visualization techniques such as this are an important addition to any statisticians toolkit, and we should not hesitate to leverage them (so long as it is appropriate, of course)."
  },
  {
    "objectID": "posts/STAN_Intro.html#what-is-stan",
    "href": "posts/STAN_Intro.html#what-is-stan",
    "title": "STAN Introduction",
    "section": "What is STAN?",
    "text": "What is STAN?\n\n\nBayesian probabilistic programming language\nMultiple posterior sampling routines\n\nHamiltonian Monte Carlo\nVariational Inference\nLaplace approximation\n\nBased on C++\nInterfaces with Python, Julia, R, and Unix Shell"
  },
  {
    "objectID": "posts/STAN_Intro.html#structure-of-a-stan-script",
    "href": "posts/STAN_Intro.html#structure-of-a-stan-script",
    "title": "STAN Introduction",
    "section": "Structure of a STAN Script",
    "text": "Structure of a STAN Script\n\nfunctions {\n  // ... function declarations and definitions ...\n}\ndata {\n  // ... declarations ...\n}\ntransformed data {\n   // ... declarations ... statements ...\n}\nparameters {\n   // ... declarations ...\n}\ntransformed parameters {\n   // ... declarations ... statements ...\n}\nmodel {\n   // ... declarations ... statements ...\n}\ngenerated quantities {\n   // ... declarations ... statements ...\n}\n\n\nAll blocks are optional, an empty string is actually a valid STAN code. Order is important, they must be in the above order Variables have scope over all subsequent blocks"
  },
  {
    "objectID": "posts/STAN_Intro.html#section---functions",
    "href": "posts/STAN_Intro.html#section---functions",
    "title": "STAN Introduction",
    "section": "Section - functions",
    "text": "Section - functions\n\n\nComplex indexing\n\nSparsely observed data\n\nGenerating quantities/structures\n\nSplines, etc.\n\nSuffixes for particular functions\n\nContaining RNG: “_rng”\nModifying target density: “_lp”\n\n\n\n\nRNG functions can only be used on data/generated quantities"
  },
  {
    "objectID": "posts/STAN_Intro.html#section---data",
    "href": "posts/STAN_Intro.html#section---data",
    "title": "STAN Introduction",
    "section": "Section - data",
    "text": "Section - data\n\n\nLikelihood data\n\nIndexing arrays\n\nAll constants\n\nArray extents\n\nCommonly used linear transforms\n\n\n\nOften need more information in this block than you think to fully specify the model."
  },
  {
    "objectID": "posts/STAN_Intro.html#section---transformed-data",
    "href": "posts/STAN_Intro.html#section---transformed-data",
    "title": "STAN Introduction",
    "section": "Section - transformed data",
    "text": "Section - transformed data\n\n\nFunctions of data variables\nOnly evaluated once\n\nPrior to sampling\n\nHelpful for book-keeping\n\nSimplify data inputs\n\nRandom data sub-samples"
  },
  {
    "objectID": "posts/STAN_Intro.html#section---parameters",
    "href": "posts/STAN_Intro.html#section---parameters",
    "title": "STAN Introduction",
    "section": "Section - parameters",
    "text": "Section - parameters\n\n\nSpecify sampled quantities\n\nVariable names\nExtents\n\nDefinitions only, no statements\nRead from underlying sampler\nCan provide initial values\n\n\n\n\nWill get more into how to provide initial values shortly"
  },
  {
    "objectID": "posts/STAN_Intro.html#section---transformed-parameters",
    "href": "posts/STAN_Intro.html#section---transformed-parameters",
    "title": "STAN Introduction",
    "section": "Section - transformed parameters",
    "text": "Section - transformed parameters\n\n\nDeterministic functions\nPart of target posterior\nEvaluated with each sample\n\nInverse transform\nLog absolute Jacobian\n\nGood for re-parameterization\n\nStability\nLatent modeling\n\n\n\n\n\nCan include data, transformed data, and parameters as inputs\nNeed inverse transform and log absolute Jacobian to be efficient, as they can greatly slow down the sampling process otherwise\nNatural scale vs sampling efficiency (NUTS and standard HMC prefer unconstrained parameters). However, gradient calculations usually dwarf these differences"
  },
  {
    "objectID": "posts/STAN_Intro.html#section---model",
    "href": "posts/STAN_Intro.html#section---model",
    "title": "STAN Introduction",
    "section": "Section - model",
    "text": "Section - model\n\n\nDefine the target posterior\n\nSum of log density functions\n\nPrior distributions on (transformed) parameters\nData/model likelihood\nMost computational expense\nORDER MATTERS\n\n\n\nIt is often important to know which subgroup of operations within the model block is consuming the majority of computation time (after deciding that efficient is insufficient). We will get to how this can be done (do not optimize unless necessary)."
  },
  {
    "objectID": "posts/STAN_Intro.html#section---generated-quantities",
    "href": "posts/STAN_Intro.html#section---generated-quantities",
    "title": "STAN Introduction",
    "section": "Section - generated quantities",
    "text": "Section - generated quantities\n\n\nExecuted after samples are generated\nFunctions of model output\n\nPredictions for new data\nSimulate new data\nExtract posterior estimates\nCalculate model fit criterion"
  },
  {
    "objectID": "posts/STAN_Intro.html#example-model---glm",
    "href": "posts/STAN_Intro.html#example-model---glm",
    "title": "STAN Introduction",
    "section": "Example Model - GLM",
    "text": "Example Model - GLM\n\ndata {\n  int N;  // Number of observations\n  int P;  // Number of fixed effect covariates\n  \n  array[N] int&lt;lower=0, upper=1&gt; Y;  // Binary outcomes\n  matrix[N, P] X;                    // Fixed effects design matrix\n}\nparameters {\n   vector[P] beta;  // Coefficients\n}\nmodel {\n   Y ~ bernoulli_logit(X * beta);\n}\n\n\nWhile there is no explicit prior on beta here, that just means that STAN is choosing the equivalent to a uniform prior under the hood"
  },
  {
    "objectID": "posts/STAN_Intro.html#running-the-model-in-r",
    "href": "posts/STAN_Intro.html#running-the-model-in-r",
    "title": "STAN Introduction",
    "section": "Running the Model in R",
    "text": "Running the Model in R\n\nfit_df = mtcars %&gt;%\n  mutate(Efficient = case_when(mpg &gt;= median(mpg) ~ 1,\n                               TRUE ~ 0)) %&gt;%\n  mutate(am = as.factor(am))\nfit_matrix = model.matrix(~cyl + disp + hp + drat + wt + am, fit_df)\n\ndata_list = list(N = nrow(mtcars), P = ncol(fit_matrix), \n                 Y = fit_df$Efficient, X = fit_matrix)\n\nmodel = sampling(\n  first_model, \n  data = data_list, \n  chains = 4, \n  iter = 1000, \n  warmup = 500, \n  # init = ,\n  # control = list(adapt_delta = , \n  #                max_treedepth = , \n  #                stepsize_jitter = ), \n  verbose = F,\n  refresh = 0\n)"
  },
  {
    "objectID": "posts/STAN_Intro.html#convergence-monitoring",
    "href": "posts/STAN_Intro.html#convergence-monitoring",
    "title": "STAN Introduction",
    "section": "Convergence Monitoring",
    "text": "Convergence Monitoring\n\ncheck_hmc_diagnostics(model)\n\n\nDivergences:\n\n\n0 of 2000 iterations ended with a divergence.\n\n\n\nTree depth:\n\n\n1212 of 2000 iterations saturated the maximum tree depth of 10 (60.6%).\nTry increasing 'max_treedepth' to avoid saturation.\n\n\n\nEnergy:\n\n\nE-BFMI indicated no pathological behavior.\n\n\n\nsummary(model)$summary[,\"Rhat\"]\n\n beta[1]  beta[2]  beta[3]  beta[4]  beta[5]  beta[6]  beta[7]     lp__ \n2.365263 2.081568 2.424803 2.459833 2.446590 1.643589 2.438779 1.039459"
  },
  {
    "objectID": "posts/STAN_Intro.html#hamiltonian-monte-carlo",
    "href": "posts/STAN_Intro.html#hamiltonian-monte-carlo",
    "title": "STAN Introduction",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\nHMC Visualization: By Justinkunimune - github.com/jkunimune/hamiltonian-mc, CC0"
  },
  {
    "objectID": "posts/STAN_Intro.html#hamiltonian-monte-carlo-continued",
    "href": "posts/STAN_Intro.html#hamiltonian-monte-carlo-continued",
    "title": "STAN Introduction",
    "section": "Hamiltonian Monte Carlo Continued",
    "text": "Hamiltonian Monte Carlo Continued\n\n\nVersion of Metropolis-Hastings\nHamiltonian Dynamics used to propose next state\n\nTrajectory with momentum\nDistribution \\(\\approx\\) potential energy field\nLeapfrog integrator stepwise approximation\n\nMomentum: reduced correlation between samples\nEnergy conservation: high acceptance probability\n\n\n\n\nLeapfrog integrator uses model gradient to take steps, why it is important to keep this efficient (analytical gradients built-in by STAN for many things)\nUse MH step to compensate for numerical issues in the Leapfrog algorithm, but acceptance probability should be high if things go well (not needed in perfect world)"
  },
  {
    "objectID": "posts/STAN_Intro.html#divergences",
    "href": "posts/STAN_Intro.html#divergences",
    "title": "STAN Introduction",
    "section": "Divergences",
    "text": "Divergences\n\n\nSimulated trajectory \\(\\neq\\) true trajectory\nGlobal step size \\(&gt;\\) true posterior geometry resolution\n\nLeapfrog first order approximataion\n\nHamiltonian departs from initial value\n\nTotal energy (kinetic + potential)\nShould be preserved along trajectory\n\nSampler WILL NOT accept samples after divergence"
  },
  {
    "objectID": "posts/STAN_Intro.html#tree-depth-warnings",
    "href": "posts/STAN_Intro.html#tree-depth-warnings",
    "title": "STAN Introduction",
    "section": "Tree Depth Warnings",
    "text": "Tree Depth Warnings\n\n\nTree depth controls number of simulation steps\n\n\\(\\leq 2^{max\\_treedepth}\\) steps\n\nPrimarily an efficiency concern\nGenerally recommended to not increase\n\nOften model misspecification"
  },
  {
    "objectID": "posts/STAN_Intro.html#est.-bayesian-fraction-of-miss.-info.",
    "href": "posts/STAN_Intro.html#est.-bayesian-fraction-of-miss.-info.",
    "title": "STAN Introduction",
    "section": "Est. Bayesian Fraction of Miss. Info.",
    "text": "Est. Bayesian Fraction of Miss. Info.\n\n\nPosterior decomposes into energy equivalence classes\nLow EBFMI indicates getting “stuck” in energy sets\n\nSTAN monitors energy during sampling\n\nInsufficiently exploring the posterior\n\nTails too large, etc"
  },
  {
    "objectID": "posts/STAN_Intro.html#geometric-intuition",
    "href": "posts/STAN_Intro.html#geometric-intuition",
    "title": "STAN Introduction",
    "section": "Geometric Intuition",
    "text": "Geometric Intuition"
  },
  {
    "objectID": "posts/STAN_Intro.html#return-to-our-example-model-outputs",
    "href": "posts/STAN_Intro.html#return-to-our-example-model-outputs",
    "title": "STAN Introduction",
    "section": "Return to our Example: Model Outputs",
    "text": "Return to our Example: Model Outputs\n\nsamples = extract(model)\n\nbeta_0 = map(1:dim(samples$beta)[1], function(x){\n  return(data.frame(beta0 = samples$beta[x,1], \n                    Sample = x))\n}) %&gt;% list_rbind()"
  },
  {
    "objectID": "posts/STAN_Intro.html#model-outputs-2",
    "href": "posts/STAN_Intro.html#model-outputs-2",
    "title": "STAN Introduction",
    "section": "Model Outputs (2)",
    "text": "Model Outputs (2)\n\nsamples = extract(model, \"beta\", permuted = FALSE)\n\nbeta_0 = map(1:dim(samples)[1], function(x){\n  return(data.frame(beta0 = samples[x,,1], \n                    Chain = 1:4, \n                    sample = x))\n}) %&gt;% list_rbind()"
  },
  {
    "objectID": "posts/STAN_Intro.html#shinystan-debugging",
    "href": "posts/STAN_Intro.html#shinystan-debugging",
    "title": "STAN Introduction",
    "section": "ShinySTAN Debugging",
    "text": "ShinySTAN Debugging\n\nlaunch_shinystan(model)"
  },
  {
    "objectID": "posts/STAN_Intro.html#shinystan---nuts-summary",
    "href": "posts/STAN_Intro.html#shinystan---nuts-summary",
    "title": "STAN Introduction",
    "section": "ShinySTAN - NUTS Summary",
    "text": "ShinySTAN - NUTS Summary"
  },
  {
    "objectID": "posts/STAN_Intro.html#shinystan---divergences",
    "href": "posts/STAN_Intro.html#shinystan---divergences",
    "title": "STAN Introduction",
    "section": "ShinySTAN - Divergences",
    "text": "ShinySTAN - Divergences"
  },
  {
    "objectID": "posts/STAN_Intro.html#shinystan---treedepth",
    "href": "posts/STAN_Intro.html#shinystan---treedepth",
    "title": "STAN Introduction",
    "section": "ShinySTAN - Treedepth",
    "text": "ShinySTAN - Treedepth"
  },
  {
    "objectID": "posts/STAN_Intro.html#shinystan---energy",
    "href": "posts/STAN_Intro.html#shinystan---energy",
    "title": "STAN Introduction",
    "section": "ShinySTAN - Energy",
    "text": "ShinySTAN - Energy"
  },
  {
    "objectID": "posts/STAN_Intro.html#shinystan---autocorrelation",
    "href": "posts/STAN_Intro.html#shinystan---autocorrelation",
    "title": "STAN Introduction",
    "section": "ShinySTAN - Autocorrelation",
    "text": "ShinySTAN - Autocorrelation"
  },
  {
    "objectID": "posts/STAN_Intro.html#how-to-update-the-model",
    "href": "posts/STAN_Intro.html#how-to-update-the-model",
    "title": "STAN Introduction",
    "section": "How to Update the Model",
    "text": "How to Update the Model\n\ndata {\n  int N;  // Number of observations\n  int P;  // Number of fixed effect covariates\n  \n  array[N] int&lt;lower=0, upper=1&gt; Y;  // Binary outcomes\n  matrix[N, P] X;                    // Fixed effects design matrix\n}\ntransformed data {\n  matrix[N, P] Q_coef = qr_thin_Q(X) * sqrt(N-1);\n  matrix[P, P] R_coef = qr_thin_R(X) / sqrt(N-1);\n  matrix[P, P] R_coef_inv = inverse(R_coef);\n}\nparameters {\n  vector[P] theta;  // Coefficients\n}\nmodel {\n  theta ~ normal(0, 100);\n  Y ~ bernoulli_logit(Q_coef * theta);\n}\ngenerated quantities {\n  vector[P] beta = R_coef_inv * theta;\n}"
  },
  {
    "objectID": "posts/STAN_Intro.html#running-the-updated-model",
    "href": "posts/STAN_Intro.html#running-the-updated-model",
    "title": "STAN Introduction",
    "section": "Running the Updated Model",
    "text": "Running the Updated Model\n\nmodel = sampling(\n  second_model, \n  data = data_list, \n  chains = 4, \n  iter = 2500, \n  warmup = 1000, \n  control = list(adapt_delta = 0.95),\n  verbose = F,\n  refresh = 0\n)"
  },
  {
    "objectID": "posts/STAN_Intro.html#updated-model-performance",
    "href": "posts/STAN_Intro.html#updated-model-performance",
    "title": "STAN Introduction",
    "section": "Updated Model Performance",
    "text": "Updated Model Performance\n\ncheck_hmc_diagnostics(model)\n\n\nDivergences:\n\n\n0 of 6000 iterations ended with a divergence.\n\n\n\nTree depth:\n\n\n0 of 6000 iterations saturated the maximum tree depth of 10.\n\n\n\nEnergy:\n\n\nE-BFMI indicated no pathological behavior.\n\n\n\nsummary(model)$summary[,\"Rhat\"]\n\ntheta[1] theta[2] theta[3] theta[4] theta[5] theta[6] theta[7]  beta[1] \n1.003796 1.003760 1.001931 1.003562 1.003605 1.003334 1.003570 1.002406 \n beta[2]  beta[3]  beta[4]  beta[5]  beta[6]  beta[7]     lp__ \n1.001364 1.003667 1.003591 1.003552 1.000757 1.003570 1.001324"
  },
  {
    "objectID": "posts/STAN_Intro.html#updated-model-visualization",
    "href": "posts/STAN_Intro.html#updated-model-visualization",
    "title": "STAN Introduction",
    "section": "Updated Model Visualization",
    "text": "Updated Model Visualization"
  },
  {
    "objectID": "posts/STAN_Intro.html#updated-geometry",
    "href": "posts/STAN_Intro.html#updated-geometry",
    "title": "STAN Introduction",
    "section": "Updated Geometry",
    "text": "Updated Geometry"
  },
  {
    "objectID": "posts/STAN_Intro.html#profiling-the-model-with-cmdstanr-1",
    "href": "posts/STAN_Intro.html#profiling-the-model-with-cmdstanr-1",
    "title": "STAN Introduction",
    "section": "Profiling the Model with CmdStanR (1)",
    "text": "Profiling the Model with CmdStanR (1)\n\ndata {\n  int N;  // Number of observations\n  int P;  // Number of fixed effect covariates\n  \n  array[N] int&lt;lower=0, upper=1&gt; Y;  // Binary outcomes\n  matrix[N, P] X;                    // Fixed effects design matrix\n}\ntransformed data {\n  matrix[N, P] Q_coef = qr_thin_Q(X) * sqrt(N-1);\n  matrix[P, P] R_coef = qr_thin_R(X) / sqrt(N-1);\n  matrix[P, P] R_coef_inv = inverse(R_coef);\n}\nparameters {\n  vector[P] theta;  // Coefficients\n}\nmodel {\n  profile(\"Priors\") {\n    target += normal_lpdf(theta| 0, 100);\n  }\n  profile(\"Likelihood\") {\n    target += bernoulli_logit_lpmf(Y| Q_coef * theta);\n  }\n}\ngenerated quantities {\n  profile(\"Generated\") {\n    vector[P] beta = R_coef_inv * theta;\n  }\n}"
  },
  {
    "objectID": "posts/STAN_Intro.html#profiling-the-model-with-cmdstanr-2",
    "href": "posts/STAN_Intro.html#profiling-the-model-with-cmdstanr-2",
    "title": "STAN Introduction",
    "section": "Profiling the Model with CmdStanR (2)",
    "text": "Profiling the Model with CmdStanR (2)\n\nmodel = cmdstan_model(\"Profile_Mod.stan\")\nfit = model$sample(data = data_list, chains = 1)\nfit$profiles()[[1]][,c(1,3,4,5,8)]\n\n\n\n        name  total_time forward_time reverse_time autodiff_calls\n1  Generated 0.000175152  0.000175152   0.00000000              0\n2 Likelihood 0.145845000  0.116126000   0.02971880         170759\n3     Priors 0.026700700  0.023262300   0.00343839         170759"
  },
  {
    "objectID": "posts/STAN_Intro.html#resource-links",
    "href": "posts/STAN_Intro.html#resource-links",
    "title": "STAN Introduction",
    "section": "Resource Links",
    "text": "Resource Links\n\nSTAN Manual\nIntroduction to HMC\nBayesian Workflow\n\n\nIntro to HMC by Michael Betancourt\nBayesian Workflow by Gelman et. al."
  },
  {
    "objectID": "posts/MSFAST_Vignette.html",
    "href": "posts/MSFAST_Vignette.html",
    "title": "MSFAST: Bayesian Multivariate, Sparse FPCA Vignette",
    "section": "",
    "text": "# General helper functions\nsource(\"MSFAST_support/Libs.R\")\nsource(\"MSFAST_support/Bases.R\")\nsource(\"MSFAST_support/PostProcess.R\")\nsource(\"MSFAST_support/Convergence.R\")\n\n# Supporting functions for MSFAST\nsource(\"MSFAST_support/MSFAST_Help.R\")\nsource(\"MSFAST_support/Resample_Scores.R\")"
  },
  {
    "objectID": "posts/MSFAST_Vignette.html#read-in-the-content-child-growth-data",
    "href": "posts/MSFAST_Vignette.html#read-in-the-content-child-growth-data",
    "title": "MSFAST: Bayesian Multivariate, Sparse FPCA Vignette",
    "section": "Read in the CONTENT Child Growth Data",
    "text": "Read in the CONTENT Child Growth Data\nFor this vignette, we analyze a subset (\\(N = 197\\)) of child growth data from the CONTENT study as included in the “refund” package. As part of CONTENT, multiple growth measures were taken at the same time for each participant and sparsely across individuals.\nCONTENT was conducted between May 2007 and February 2011 in Las Pampas de San Juan Miraflores and Nuevo Paraíso, two peri-urban shanty towns located on the southern edge of Lima City in Peru. The shanty towns had approximately \\(40{,}000\\) residents with \\(25\\)% of the population under the age of \\(5\\) (W. Checkley et al. 1998; William Checkley et al. 2003).\nA simple census was conducted to identify pregnant women and children less than \\(3\\) months old. Eligible newborns and pregnant women were randomly selected and invited to participate in the study (at most one newborn per household). The longitudinal cohort study aimed to assess whether Helicobacter pylori (H. pylori) infection adversely affects the growth in children less than \\(2\\) years of age (Jaganath et al. 2014; Crainiceanu et al. 2024).\nThe study aimed to collect measures weekly until the child was \\(3\\) months old, biweekly between \\(3\\) and \\(11\\) months, and once monthly afterwards. Some visits were missed or canceled, contributing to the sparse data structure.\nWe read in the data using standard Tidyverse syntax, focusing on the length and weight z-scores relative to the age- and sex-specific World Health Organization standards.\n\ndata(content)\n\ncontent = content %&gt;%\n  select(id, ma1fe0, agedays, zlen, zwei)"
  },
  {
    "objectID": "posts/MSFAST_Vignette.html#visualization-of-the-sparse-structure",
    "href": "posts/MSFAST_Vignette.html#visualization-of-the-sparse-structure",
    "title": "MSFAST: Bayesian Multivariate, Sparse FPCA Vignette",
    "section": "Visualization of the Sparse Structure",
    "text": "Visualization of the Sparse Structure\nWe now visualize the observation times by gender for the entire dataset and the observed data for four participants (two randomly chosen and two chosen for holdout to illustrate dynamic prediction). These visualizations borrow strongly from the section in “Functional Data Analysis with R” on sparsely observed functional data, which includes discussion of the CONTENT dataset. The first visualization aims to illustrate the general structure of the sparsity in observations.\n\nset.seed(37578) # Essential for reproducibility\nfor_prediction = c(73, 112) # Participants for dynamic prediction\n\nvis_ppts = c(sample(unique(content %&gt;% filter(ma1fe0 == 0) %&gt;% pull(id)), 1), \n             for_prediction[1], \n             sample(unique(content %&gt;% filter(ma1fe0 == 1) %&gt;% pull(id)), 1), \n             for_prediction[2])\n\ncontent %&gt;%\n  select(id, ma1fe0, agedays) %&gt;%\n  mutate(Gender = case_when(ma1fe0 == 1 ~ \"Male\", \n                            TRUE ~ \"Female\")) %&gt;%\n  group_by(Gender, id) %&gt;%\n  nest(data = agedays) %&gt;%\n  ungroup() %&gt;%\n  group_by(Gender) %&gt;%\n  mutate(pptid = row_number()) %&gt;%\n  ungroup() %&gt;%\n  unnest(data) %&gt;%\n  mutate(coloring = case_when(id %in% vis_ppts ~ \"Observed\", \n                              TRUE ~ \"Excluded\")) %&gt;%\n  ggplot(aes(x = agedays, y = pptid)) + \n  geom_point(aes(color = coloring), size = 1) + \n  theme_bw() + \n  facet_wrap(.~Gender) + \n  scale_color_manual(values = c(\"Observed\" = \"darkorange\", \n                                \"Excluded\" = \"steelblue\")) + \n  labs(x = \"Age (Days)\", y = \"Participant\", title = \"\") +\n  theme(axis.text.y = element_blank(), \n        axis.ticks.y = element_blank(), \n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nFocusing on just four participants, two of each gender highlighted in the above observations plot, we can better visualize the actual bivariate patterns within the data.\n\nlabs = c(paste0(c(\"Female\", \"Male\"), \" Ppt. 1\"), \n         paste0(c(\"Female\", \"Male\"), \" Ppt. 2\"))\ncontent %&gt;%\n  filter(id %in% vis_ppts) %&gt;%\n  mutate(new_id = case_when(id == vis_ppts[1] ~ labs[1], \n                            id == vis_ppts[2] ~ labs[3], \n                            id == vis_ppts[3] ~ labs[2], \n                            TRUE ~ labs[4]), \n         new_id = factor(new_id, levels = labs)) %&gt;%\n  select(new_id, id, agedays, zlen, zwei) %&gt;%\n  pivot_longer(-c(new_id, id, agedays), names_to = \"Measure\", values_to = \"Y\") %&gt;%\n  mutate(Measure = case_when(Measure == \"zlen\" ~ \"Length\", \n                             TRUE ~ \"Weight\")) %&gt;%\n  ggplot(aes(x = agedays, y = Y, group = Measure, color = Measure)) + \n  geom_point() + \n  facet_wrap(.~new_id, ncol = 2) + \n  theme_bw() + \n  labs(x = \"Age (Days)\", y = \"Z-Score\", title = \"\") + \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/MSFAST_Vignette.html#fitting-msfast",
    "href": "posts/MSFAST_Vignette.html#fitting-msfast",
    "title": "MSFAST: Bayesian Multivariate, Sparse FPCA Vignette",
    "section": "Fitting MSFAST",
    "text": "Fitting MSFAST\nWe first remove two participants from the dataset that will be used to fit the model. We will illustrate dynamic prediction using the data from these participants later.\n\ncontent_fit = content %&gt;%\n  filter(!(id %in% for_prediction)) %&gt;%\n  select(-c(ma1fe0))\n\ncontent_predict = content %&gt;%\n  filter(id %in% for_prediction) %&gt;%\n  select(-c(ma1fe0)) %&gt;%\n  rename(Arg = agedays)\n\nAt this point, we form the input data list required by MSFAST, described in the “Bayesian implementation in STAN” section of the manuscript.\n\n# Pivot to long and adjust names\nfit_df = content_fit %&gt;%\n  pivot_longer(-c(id, agedays),\n               names_to = \"Covar\",\n               values_to = \"Y\") %&gt;%\n  mutate(Var = case_when(Covar == \"zlen\" ~ 1, # Replace names with indices\n                         TRUE ~ 2)) %&gt;%\n  select(-c(Covar)) %&gt;%\n  reset_id() %&gt;% # Reset participant ids to sequential integers\n  rename(Subj = id, Arg = agedays)\n\n# Find observation time indices S and append\nS_df = data.frame(Arg = sort(unique(fit_df$Arg)))\nS_df$S = 1:nrow(S_df)\nfit_df = inner_join(fit_df, S_df, by = \"Arg\")\ncontent_predict = inner_join(content_predict, S_df, by = \"Arg\")\n\n# Translate domain to (0, 1)\ndomain = S_df$Arg\nscaled_domain = (domain - min(domain)) / (max(domain) - min(domain))\n\nN = n_distinct(fit_df$Subj)   # Number of participants\nQ = 20                        # Spline basis dimension\nK = 4                         # Number of principal components\nP = n_distinct(fit_df$Var)    # Number of unique covariates (2 here)\n\n# Function to coalesce inputs (basis is orthogonalized B-splines by default)\ndata_list = MSFAST_datalist(fit_df, N = N, K = K, Q = Q, scaled_domain, \n                            scale = T)\n\nWe can finally fit the model using RSTAN as follows.\n\nfit_joint = stan(\"MSFAST_support/MSFAST.stan\", \n                 data = data_list, \n                 chains = 4, \n                 cores = 4, \n                 warmup = 2000, \n                 iter = 3000, \n                 control = list(max_treedepth = 12))\n\nNote that the above block does not leverage parallelization over covariates through multithreading. To leverage this computational speed-up (after ensuring sufficient available resources), one can use “Sys.setenv(STAN_NUM_THREADS=X)” for \\(\\text{X} &gt; 1\\) and the parallelized STAN implementation."
  },
  {
    "objectID": "posts/MSFAST_Vignette.html#evaluating-fit-and-aligning-results",
    "href": "posts/MSFAST_Vignette.html#evaluating-fit-and-aligning-results",
    "title": "MSFAST: Bayesian Multivariate, Sparse FPCA Vignette",
    "section": "Evaluating Fit and Aligning Results",
    "text": "Evaluating Fit and Aligning Results\nWe first fit mFACEs, extracting the estimates provided by this method to be used as our fixed point \\(\\widetilde{\\boldsymbol{\\Phi}}\\) for the Procrustes-based alignment of posterior FPC samples described in the manuscript.\n\n# Data in list format\nface_data = list(content %&gt;% \n                   filter(!(id %in% for_prediction)) %&gt;%\n                   select(agedays, id, zlen) %&gt;%\n                   rename(argvals = agedays, \n                          subj = id, y = zlen), \n                 content %&gt;%\n                   filter(!(id %in% for_prediction)) %&gt;%\n                   select(agedays, id, zwei) %&gt;%\n                   rename(argvals = agedays, \n                          subj = id, y = zwei))\n\n# Call to function\nface_fit = mface.sparse(face_data, argvals.new = domain, knots = Q-3,\n                        knots.option = \"quantile\", pve = 0.999)\n\n# Extract first K FPC estimates as fixed point matrix\nphi_tilde = face_fit$eigenfunctions[,1:K] \n\nWe next check the Gelman-Rubin statistics for all relevant model components (after alignment). This is done automatically using the “RHat_FAST()” function as follows. This function calculates the median and max RHat observed for each FPC/covariate grouping within parameter families. The results below indicate that all parameters have converged according to the heuristic RHat &lt; 1.05 threshold.\n\nrhats = RHat_FAST(fit_joint,   # RSTAN object \n                  data_list,   # List of constants\n                  data_list$B, # Orthogonal spline basis matrix\n                  phi_tilde)   # Fixed point for rotational alignment\n\n# Scores, grouped by FPC\nrhats$Score\n\n# A tibble: 4 × 3\n  FPC_Num Med_RHat Max_RHat\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 FPC 1       1.02     1.04\n2 FPC 2       1.00     1.01\n3 FPC 3       1.00     1.01\n4 FPC 4       1.00     1.02\n\n# FPCs, grouped by FPC/functional component\nrhats$Func\n\n# A tibble: 5 × 3\n  Function Med_RHat Max_RHat\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 FPC 1        1.00     1.01\n2 FPC 2        1.00     1.00\n3 FPC 3        1.00     1.02\n4 FPC 4        1.01     1.03\n5 Mu           1.00     1.00\n\n# Fixed effect smoothing parameters, grouped by covariate\nrhats$Mu_Smoothing\n\n  Function     RHat\n1      Mu1 1.001001\n2      Mu2 1.001592\n\n# FPC smoothing parameters, grouped by FPC and covariate\nrhats$FPC_Smoothing\n\n# A tibble: 8 × 3\n  Var   FPC    RHat\n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 Var 1 FPC 1  1.02\n2 Var 1 FPC 2  1.01\n3 Var 1 FPC 3  1.00\n4 Var 1 FPC 4  1.00\n5 Var 2 FPC 1  1.00\n6 Var 2 FPC 2  1.01\n7 Var 2 FPC 3  1.01\n8 Var 2 FPC 4  1.00\n\n# Variance components, no grouping\nrhats$Variances\n\n   Element     RHat\n1 Lambda_1 1.000116\n2 Lambda_2 1.001444\n3 Lambda_3 1.007298\n4 Lambda_4 1.027927\n5   Sigma2 1.000165\n\n\nWe can extract and align the posterior samples, leveraging the Procrustes-based procedure described in the manuscript, using the provided suite of post-processing functions as follows.\n\n# Can adjust the observation points at which inference is performed\nbasis_mat = FAST_B(\"B\", Q, scaled_domain)\n\n# Extract FPCs, corresponding weights, fixed effect weights, and scores\nobjects = FAST_extract(fit_joint, basis_mat, data_list)\n\n# Align samples using Procrustes-based procedure\nalign = procrust_WEI(objects$Weights, basis_mat, P, phi_tilde, objects$Score)\n\n# Extract FPC estimates and credible intervals\nEF_ests = FPC_Est_WEI(align$Weight, basis_mat, P, domain, phi_tilde)\nEF_bounds = FPC_CI(align$EF, domain, P)\n\nWe next evaluate the proportion of global variability explained to ensure the number of FPCs \\(K\\) was chosen appropriately. The final values are above 95% for all samples, seeming to indicate that \\(K = 4\\) is sufficient for this data.\n\n# True data in sparse matrix form\ndata_matrix = fit_df %&gt;%\n  pivot_wider(names_from = Subj, values_from = Y) %&gt;%\n  arrange(Var, Arg) %&gt;%\n  select(-c(Var, Arg, S)) %&gt;%\n  as.matrix() %&gt;%\n  t()\nmask = is.na(data_matrix)\ndata_var = var(data_matrix[!mask])\n\n# Modeled smooths in sparse matrix of dim. samples x ppts x obs\nsamples_matrix = Smooth_Raw(objects$Mu, align$EF, align$Score, data_list)\nn_samp = dim(samples_matrix)[1]\nvar_expl = rep(0, n_samp)\nfor (j in 1:n_samp) {\n  model_matrix = samples_matrix[j, , ]\n  resids = data_matrix - model_matrix\n  var_expl[j] = 1 - var(resids[!mask]) / data_var # 1 - RSS/TSS across covariates\n}\n\n# Summarize global variance explained over posterior samples\nsummary(var_expl)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9501  0.9509  0.9511  0.9511  0.9512  0.9520"
  },
  {
    "objectID": "posts/MSFAST_Vignette.html#visualizing-multivariate-fpcs",
    "href": "posts/MSFAST_Vignette.html#visualizing-multivariate-fpcs",
    "title": "MSFAST: Bayesian Multivariate, Sparse FPCA Vignette",
    "section": "Visualizing Multivariate FPCs",
    "text": "Visualizing Multivariate FPCs\nWe now visualize the multivariate FPCs according to MSFAST. All estimates are accompanied by the associated equal-tailed 95% credible intervals. For a point of reference, we also include the FPC estimates from mFACEs, though these estimates do not include any uncertainty quantification.\n\n# Format MSFAST estimates\n{\n  MSFAST_FPC = left_join(EF_bounds, EF_ests %&gt;% rename(Func = FPC_Val)) %&gt;%\n    mutate(FuncName = paste0(\"phi[\", substring(FPC_Num, 5), \"](t)\")) %&gt;%\n    mutate(Method = \"MSFAST\", \n           Method = factor(Method, levels = c(\"MSFAST\", \"mFACEs\")), \n           Covar = case_when(Var == 1 ~ \"Length\", \n                             TRUE ~ \"Weight\"))\n}\n\nJoining with `by = join_by(Arg, Var, FPC_Num)`\n\n# Format mFACEs estimates for comparison\n{\n  FACE_FPC = data.frame(phi_tilde)\n  colnames(FACE_FPC) = paste0(\"phi[\", 1:K, \"](t)\")\n  FACE_FPC$Arg = rep(face_fit$argvals.new, 2)\n  FACE_FPC$Covar = rep(c(\"Length\", \"Weight\"), each = length(face_fit$argvals.new))\n  FACE_FPC = FACE_FPC %&gt;%\n    pivot_longer(-c(Arg, Covar), names_to = \"FuncName\", values_to = \"FACE\") %&gt;%\n    mutate(Method = \"mFACEs\", \n           Method = factor(Method, levels = c(\"MSFAST\", \"mFACEs\")))\n}\n\n# Visualize the results\n{\n  MSFAST_FPC %&gt;%\n    ggplot(aes(x = Arg, color = Method, fill = Method)) + \n    geom_ribbon(aes(ymin = LB, ymax = UB), linewidth = 0, alpha = 0.3) +\n    geom_line(aes(y = Func)) + \n    geom_line(data = FACE_FPC, aes(y = FACE)) +\n    facet_grid(Covar~FuncName, labeller = label_parsed) +\n    theme_bw() + \n    labs(x = \"Age (days)\", y = \"Eigenfunction\")\n}\n\n\n\n\n\n\n\n\nWe can also examine the eigenvalue posterior distributions for each of the FPCs estimated above, summarized with corresponding credible intervals in the following table. Again, we use the results from mFACEs for comparison.\n\n# MSFAST eigenvalues from score variances\nMSFAST_EV = map(align$Score, function(x) {\n  ev_sample = apply(x, 2, var)\n  return(data.frame(\n    FPC = paste0(\"Phi_\", 1:K, \"(t)\"),\n    EV = ev_sample\n  ))\n}) %&gt;% list_rbind() %&gt;%\n  group_by(FPC) %&gt;%\n  summarize(`Posterior Mean Lambda` = round(mean(EV), 2), \n            `CI Lower` = round(quantile(EV, probs = c(0.025)), 2), \n            `CI Upper` = round(quantile(EV, probs = c(0.975)), 2))\n\n# mFACEs eigenvalues from model output\nFACE_EV = data.frame(FPC = paste0(\"Phi_\", 1:K, \"(t)\"), \n                     EV = round(face_fit$eigenvalues[1:K], 2)) %&gt;%\n  rename(`mFACEs Lambda` = EV)\n\ninner_join(MSFAST_EV, FACE_EV, by = \"FPC\")\n\n# A tibble: 4 × 5\n  FPC      `Posterior Mean Lambda` `CI Lower` `CI Upper` `mFACEs Lambda`\n  &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;\n1 Phi_1(t)                    1.46       1.37       1.55            1.37\n2 Phi_2(t)                    0.24       0.21       0.27            0.35\n3 Phi_3(t)                    0.18       0.16       0.21            0.1 \n4 Phi_4(t)                    0.2        0.14       0.28            0.04\n\n\nAs can be seen above, MSFAST does not agree with mFACEs on the variability explained in the later FPCs, particularly \\(\\boldsymbol{\\Phi}_3(t)\\) and \\(\\boldsymbol{\\Phi}_4(t)\\). Given the tendency of MSFAST to better recover lower signal FPCs, this descrepancy is likely cause for re-ordering of the FPCs during visualization."
  },
  {
    "objectID": "posts/MSFAST_Vignette.html#dynamic-prediction",
    "href": "posts/MSFAST_Vignette.html#dynamic-prediction",
    "title": "MSFAST: Bayesian Multivariate, Sparse FPCA Vignette",
    "section": "Dynamic Prediction",
    "text": "Dynamic Prediction\nWe aim to predict the latent trajectories for the two held-out participants. We iteratively update predictions using these subjects’ data observed up to 150, 300, and 450 days, mimicking the dynamic prediction task. In all cases, we limit trajectory predictions to 500 days, as there is little data to draw from past this point. To form these predictions, we will need to first define relevant constants.\n\n# Predict using data up to 150, 300, and 450 days\nthresholds = 1:3 * 150\n\n# Predict trajectory only up to 500 days\nmax_project = 500\n\n# Object containing posterior samples\nsamples = extract(fit_joint)\n\n# Basis matrix for FPCs when P = 2\nkB = kronecker(diag(P), basis_mat)\n\n# Number of unique observed time points\nM = nrow(basis_mat)\n\nWe can now actually form the trajectory predictions by sampling the subject-specific scores given their available data and the posterior samples of the population-level parameters.\n\ntrajectories = map(thresholds, function(age_thresh) { # Predictions at each threshold\n  sub_traj = map(for_prediction, function(sid) { # Predictions for each participant\n    \n    # Reformat data and standardize\n    subset = content_predict %&gt;%\n      filter(Arg &lt;= age_thresh & id == sid) %&gt;%\n      pivot_longer(-c(id, Arg, S), names_to = \"Metric\", values_to = \"Val\") %&gt;%\n      mutate(Var = case_when(Metric == \"zlen\" ~ 1,\n                             TRUE ~ 2)) %&gt;%\n      left_join(data_list$consts, by = \"Var\") %&gt;%\n      mutate(Val = (Val - mu_Y) / sd_Y)\n    \n    # Structure observations\n    Yi = list(length = subset %&gt;% \n                filter(Metric == \"zlen\") %&gt;%\n                pull(Val),\n              weight = subset %&gt;%\n                filter(Metric == \"zwei\") %&gt;%\n                pull(Val))\n    \n    # Orthogonal spline evaluated at the observed points\n    Bi = list(length = basis_mat[subset %&gt;% filter(Metric == \"zlen\") %&gt;% pull(S), ], \n              weight = basis_mat[subset %&gt;% filter(Metric == \"zwei\") %&gt;% pull(S), ])\n    \n    # Sample the scores conditional on population estimates and available data\n    scores = sample_scores(Yi, Bi, samples) %&gt;%\n      select(-c(Sample)) %&gt;%\n      as.matrix()\n    \n    # Calculate trajectories from sampled scores and population-level samples\n    person_traj = map(1:dim(scores)[1], function(x) {\n      smooths = kB %*% (samples$w_mu[x,] + samples$Psi[x, , ] %*% scores[x, ])\n      \n      return(data.frame(Sample = x, Arg = domain,\n                        zlen = smooths[1:M],\n                        zwei = smooths[(M + 1):(2 * M)]))\n    }) %&gt;% list_rbind() %&gt;%\n      pivot_longer(-c(Arg, Sample),\n                   names_to = \"Metric\",\n                   values_to = \"Traj\")\n    \n    return(person_traj %&gt;% mutate(id = sid))\n  }) %&gt;% list_rbind() %&gt;%\n    filter(Arg &lt;= max_project) %&gt;% # Only project to 500 days\n    mutate(Metric = case_when(Metric == \"zlen\" ~ \"Length\",\n                              TRUE ~ \"Weight\"))\n  \n  return(\n    sub_traj %&gt;% # Summarize trajectories by point-wise mean and credible intervals\n      group_by(id, Metric, Arg) %&gt;%\n      summarize(Est = mean(Traj),\n                UB = quantile(Traj, probs = c(0.975)),\n                LB = quantile(Traj, probs = c(0.025))) %&gt;%\n      mutate(id = paste0(\"Ppt. \", id)) %&gt;%\n      mutate(Data_Max = paste0(\"Age~(days)&lt;=\", age_thresh))\n  )\n}) %&gt;%\n  list_rbind()\n\nTo conclude formatting these predictions, we scale them according to the mean and standard deviation of the observed measures and arrange the thresholds for viewing.\n\ntrajectories = trajectories %&gt;%\n  mutate(Var = case_when(Metric == \"Length\" ~ 1,\n                         TRUE ~ 2)) %&gt;%\n  left_join(data_list$consts, by = \"Var\") %&gt;%\n  mutate(Est = Est * sd_Y + mu_Y, \n         UB = UB * sd_Y + mu_Y, \n         LB = LB * sd_Y + mu_Y) %&gt;%\n  select(-c(Var)) %&gt;% \n  mutate(Data_Max = factor(Data_Max, levels = paste0(\"Age~(days)&lt;=\", thresholds)))\n\nWe next format the raw data for visualization alongside the predicted trajectories.\n\n# Corresponding real data\nreal_data = map(thresholds, function(age_thresh) {\n  return(\n    content_predict %&gt;%\n      filter(Arg &lt;= age_thresh) %&gt;%\n      pivot_longer(-c(id, Arg, S), names_to = \"Metric\", values_to = \"Val\") %&gt;%\n      mutate(\n        id = paste0(\"Ppt. \", id),\n        Metric = case_when(Metric == \"zlen\" ~ \"Length\",\n                           TRUE ~ \"Weight\"),\n        Data_Max = paste0(\"Age~(days)&lt;=\", age_thresh)\n      )\n  )\n}) %&gt;%\n  list_rbind() %&gt;%\n  mutate(Data_Max = factor(Data_Max, levels = paste0(\"Age~(days)&lt;=\", thresholds)))\n\nWe can finally visualize the predictions with the data used to form them, with projected trajectories and their uncertainty shifting with the introduction of additional data. Perhaps most noticeably, the credible interval width decreases substantially as more data is included.\n\ntrajectories %&gt;%\n  ggplot(aes(x = Arg, group = Metric, color = Metric, fill = Metric)) +\n  geom_line(aes(y = Est)) +\n  geom_ribbon(aes(ymin = LB, ymax = UB), alpha = 0.2) +\n  geom_point(data = real_data, aes(y = Val)) +\n  theme_bw() +\n  scale_x_continuous(lim = c(0, 505)) +\n  coord_cartesian(ylim = c(-2.5, 1)) +\n  facet_grid(Data_Max ~ id,\n             labeller = labeller(Data_Max = label_parsed),\n             scales = \"free_y\") +\n  labs(x = \"Age (days)\", y = \"Z-Score\")\n\n\n\n\n\n\n\n\nThis document just visualizes a particular case of applying the “MSFAST” approach to perform fully-Bayesian Functional PCA for sparse, multivariate data. All supporting STAN and R code here is designed to be flexible, such that it may directly be applied to any other similarly structured problem."
  },
  {
    "objectID": "posts/FAST_Vignette.html",
    "href": "posts/FAST_Vignette.html",
    "title": "FAST: Bayesian FPCA Vignette",
    "section": "",
    "text": "source(\"FAST_support/Libs.R\")\nsource(\"FAST_support/Bases.R\")\nsource(\"FAST_support/Convergence.R\")\nsource(\"FAST_support/PostProcess.R\")\nsource(\"FAST_support/FAST_Help.R\")"
  },
  {
    "objectID": "posts/FAST_Vignette.html#read-in-the-nhanes-accelerometry-data",
    "href": "posts/FAST_Vignette.html#read-in-the-nhanes-accelerometry-data",
    "title": "FAST: Bayesian FPCA Vignette",
    "section": "Read in the NHANES Accelerometry Data",
    "text": "Read in the NHANES Accelerometry Data\nFor this brief example analysis, we leverage the objective physical activity data from wrist-worn accelerometry collected as part of NHANES 2011-2014. This dataset is made publicly available as part of “Functional Data Analysis with R” and can be downloaded from the accompanying website. We take a random subsample of 200 individuals subsampled every 10 minutes during the day (144 total measurements) for the sake of time.\n\ndata_path = \"http://www.ciprianstats.org/sites/default/files/nhanes/nhanes_fda_with_r.rds\"\ndownload.file(data_path, \"nhanes.rds\", mode = \"wb\")\naccel_data = readRDS(\"nhanes.rds\")\nfile.remove(\"nhanes.rds\")\n\nset.seed(12345)\nchosen_idxs = sample(1:nrow(accel_data), 200)\naccel_mat = unAsIs(accel_data$MIMS)[chosen_idxs, ]\naccel_df = data.frame(accel_mat)\ncolnames(accel_df) = 1:1440\naccel_df$ID = accel_data$SEQN[chosen_idxs]\naccel_df = accel_df %&gt;%\n  pivot_longer(-c(ID), names_to = \"MoD\", values_to = \"MIMS\") %&gt;%\n  mutate(MoD = as.numeric(MoD), \n         Window = (MoD - 1) %/% 10) %&gt;%\n  group_by(ID, Window) %&gt;%\n  summarize(MIMS = mean(MIMS)) %&gt;%\n  mutate(MoD = Window * 10)"
  },
  {
    "objectID": "posts/FAST_Vignette.html#visualization-of-data-structure",
    "href": "posts/FAST_Vignette.html#visualization-of-data-structure",
    "title": "FAST: Bayesian FPCA Vignette",
    "section": "Visualization of Data Structure",
    "text": "Visualization of Data Structure\nNow that we have the data, we perform quick visualizations to get an idea of the types of patterns present. We first randomly select 5 participant’s MIMS (monitor-independent movement summary) curves.\n\nset.seed(0935867)\nchosen_ids = sample(unique(accel_df$ID), 6)\n\naccel_df %&gt;%\n  filter(ID %in% chosen_ids) %&gt;%\n  mutate(HoD = MoD/60, ID = paste0(\"ID: \", ID)) %&gt;%\n  ggplot(aes(x = HoD, y = MIMS, group = ID)) + \n  geom_line() +\n  facet_wrap(.~ID) + \n  theme_bw() + \n  labs(x = \"Hour of the Day\", y = \"MIMS\")\n\n\n\n\n\n\n\n\nWe can alternatively visualize the entire population using a heatmap structure, where each row is a participant and darker colors correspond to higher activity levels (as in “Functional Data Analysis with R”).\n\naccel_df %&gt;%\n  mutate(ID = factor(ID), \n         HoD = MoD/60) %&gt;%\n  ggplot(aes(x = HoD, y = ID)) + \n  geom_tile(aes(fill = MIMS)) +\n  theme_bw() + \n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank()) +\n  scale_fill_gradient(low = \"white\", high = \"black\") + \n  labs(x = \"Hour of the Day\", y = \"Participant\")\n\n\n\n\n\n\n\n\nThis visualization indicates that activity is strongest between the hours of 8AM and 10PM, as would be expected. However, there is substantial heterogeneity both in when individuals start and end their days, as well as the amount of activity they engage in on average throughout the course of the day."
  },
  {
    "objectID": "posts/FAST_Vignette.html#fitting-fast-bayesian-fpca",
    "href": "posts/FAST_Vignette.html#fitting-fast-bayesian-fpca",
    "title": "FAST: Bayesian FPCA Vignette",
    "section": "Fitting FAST Bayesian FPCA",
    "text": "Fitting FAST Bayesian FPCA\nWe first retrieve the matrix representation of the full dataset using the appropriate wrangling. We then define the constants, using spline basis of dimension \\(Q = 25\\) and \\(K = 3\\) FPCs. We provide these values, along with the input data matrix, to the “FAST_datalist()” function, which concludes collating all requisite inputs to the FAST STAN implementation. This includes generating the spline bases for the fixed and random effects functions (b-splines and orthogonal splinets, respectively), along with their associated quadratic penalty matrices. All of these elements are collated into the STAN input list.\n\n# Place data in wide matrix format\nY_mat = accel_df %&gt;% \n  ungroup() %&gt;%\n  select(-c(Window)) %&gt;%\n  pivot_wider(names_from = MoD, values_from = MIMS) %&gt;%\n  select(-c(ID)) %&gt;% as.matrix()\n\n# Define constants\nQ = 25\nK = 3\n\n# Collate list of arguments\nDomain = sort(unique(accel_df$MoD))/60\nScaled_Domain = (Domain - min(Domain))/(max(Domain) - min(Domain))\ndata_list = FAST_datalist(Y_mat, Q, K, Scaled_Domain)\n\nWith all requisite inputs generated, we can finally make the call to RSTAN in order to fit the model, accomplished as follows.\n\nfit_mod = stan(file = \"FAST_support/FAST.stan\",\n               data = data_list, \n               chains = 4, \n               cores = 4, \n               warmup = 3000, \n               iter = 5000)"
  },
  {
    "objectID": "posts/FAST_Vignette.html#evaluating-fit-and-aligning-results",
    "href": "posts/FAST_Vignette.html#evaluating-fit-and-aligning-results",
    "title": "FAST: Bayesian FPCA Vignette",
    "section": "Evaluating Fit and Aligning Results",
    "text": "Evaluating Fit and Aligning Results\nWe first extract and summarize all relevant model parameters, aligning the FPCs and scores according to sign and order.\n\nobjects = FAST_extract(fit_mod, data_list$B_FE, \n                       data_list$B_RE, Domain, data_list)\n\nalign = align_weights(objects$Weights, objects$Score, data_list$B_RE)\nscores_samples = out_Score(align$Score)\n    \nEF_CI = CI_EF(align$EF, Domain)\nEF_est = Psi_SVD_FPC(align$Weights, data_list$B_RE, Domain, K)\nMu_df = out_FE(objects$Mu, Domain) %&gt;%\n  group_by(Arg) %&gt;%\n  summarize(Est = mean(Mu), \n            LB = quantile(Mu, probs = c(0.025)), \n            UB = quantile(Mu, probs = c(0.975)))\n\nThe next step is assessment of model convergence using Gelman-Rubin RHat statistics. Principal component sign and ordering are aligned prior to calculating these sampling diagnostics. These measures are calculated using the “RHat_FAST()” function as follows. This function calculates the median and max RHat observed for each FPC/covariate grouping within parameter families. The results below indicate that all parameters have converged according to the heuristic RHat &lt; 1.05 threshold.\n\nrhats = RHat_FAST(fit_mod, data_list, align$EF[[1]])\n\n# FPCs, grouped by FPC/functional component\nrhats$Func\n\n# A tibble: 4 × 3\n  Function Med_RHat Max_RHat\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 FPC 1        1.02     1.02\n2 FPC 2        1.02     1.02\n3 FPC 3        1.00     1.00\n4 Mu           1.01     1.01\n\n# Scores, grouped by FPC\nrhats$Score\n\n# A tibble: 3 × 3\n  FPC_Num Med_RHat Max_RHat\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 FPC 1       1.01     1.03\n2 FPC 2       1.01     1.02\n3 FPC 3       1.00     1.01\n\n# Smoothing parameters, grouped by functional component\nrhats$Smoothing_Params\n\n  Function      RHat\n1    FPC 1 1.0002062\n2    FPC 2 0.9997524\n3    FPC 3 0.9997878\n4       Mu 0.9997878\n\n# Variance components (eigenvalues and noise variance)\nrhats$Variances\n\n   Element     RHat\n1 Lambda_1 1.007618\n2 Lambda_2 1.012016\n3 Lambda_3 1.000078\n4   Sigma2 1.000626\n\n\nTo evaluate the adequacy of \\(K = 3\\) principal components, we evaluate the posterior distribution of variability explained. Results indicate that \\(K = 3\\) principal components explains \\(\\approx 80\\)% of the variability in the observed data.\n\n# True data in sparse matrix form\ndata_var = var(as.vector(Y_mat))\n\n# Modeled smooths in sparse matrix of dim. samples x ppts x obs\nsamples_matrix = Smooth_Raw(objects$Mu, align$EF, align$Score, data_list)\nn_samp = dim(samples_matrix)[1]\nvar_expl = rep(0, n_samp)\nfor (j in 1:n_samp) {\n  model_matrix = samples_matrix[j, , ]\n  resids = Y_mat - model_matrix\n  var_expl[j] = 1 - var(as.vector(resids)) / data_var # 1 - RSS/TSS across covariates\n}\n\n# Summarize global variance explained over posterior samples\nsummary(var_expl)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7803  0.7811  0.7813  0.7813  0.7815  0.7822"
  },
  {
    "objectID": "posts/FAST_Vignette.html#visualizing-principal-components",
    "href": "posts/FAST_Vignette.html#visualizing-principal-components",
    "title": "FAST: Bayesian FPCA Vignette",
    "section": "Visualizing Principal Components",
    "text": "Visualizing Principal Components\nWith FAST fit, we can now visualize the extracted and aligned model components: fixed effects function \\(\\mu(t)\\), eigenfunctions \\(\\phi_k(t)\\), and eigenvalues \\(\\lambda_k\\). We first visualize the fixed effects mean \\(\\mu(t)\\), displaying the posterior mean estimate, equal-tail 95% credible interval, and 3 posterior samples.\n\nchosen_samples = sample(length(objects$Mu), 3)\nfunc_samples = map(chosen_samples, function(idx){\n  out_df = data.frame(Mu = objects$Mu[[idx]],\n                      Arg = Domain, Sample = idx)\n  return(out_df)\n}) %&gt;% list_rbind()\n\nMu_df %&gt;%\n  ggplot(aes(x = Arg)) + \n  geom_line(aes(y = Est)) + \n  geom_ribbon(aes(ymin = LB, ymax = UB), alpha = 0.1) + \n  geom_line(data = func_samples, aes(y = Mu, group = Sample), color = \"red\", alpha = 0.5) +\n  theme_bw() + \n  labs(x = \"Hour of the Day\", y = parse(text = \"mu(t)~Estimate\"))\n\n\n\n\n\n\n\n\nWe can similarly visualize the samples of eigenfunctions \\(\\phi_k(t)\\), again including the estimate, equal-tail 95% credible interval, and 3 posterior samples in red.\n\nfunc_sample = map(chosen_samples, function(idx){\n  out_df = FPC_df(align$EF[[idx]], Domain) %&gt;%\n    mutate(Sample = idx, \n           EFLab = paste0(\"phi[\", substring(FPC_Num, 4), \"](t)\"))\n  return(out_df)\n}) %&gt;% list_rbind()\n\nleft_join(EF_est, EF_CI) %&gt;%\n  mutate(EFLab = paste0(\"phi[\", substring(FPC_Num, 4), \"](t)\")) %&gt;%\n  ggplot(aes(x = Arg)) + \n  geom_line(aes(y = FPC_Val)) +\n  geom_ribbon(aes(ymin = LB, ymax = UB), alpha = 0.1) + \n  geom_line(data = func_sample, aes(y = FPC_Val, group = Sample), color = \"red\", alpha = 0.5) +\n  facet_wrap(.~EFLab, labeller = label_parsed) + \n  theme_bw() + \n  labs(x = \"Hour of the Day\", y = \"Eigenfunction Estimate\")\n\n\n\n\n\n\n\n\nWe can also evaluate the variability decomposition by plotting the estimated eigenvalues \\(\\lambda_k\\) sequentially with their equal-tail 95% credible intervals.\n\nlabs = paste0(\"lambda[\", 1:K, \"]\")\nnames(labs) = paste0(\"FPC \", 1:K)\n\nmap(1:length(align$Score), function(x){\n  eigenvals = apply(align$Score[[x]], 2, var)\n  return(data.frame(FPC = paste0(\"FPC \", 1:K), \n                    EVal = eigenvals))\n}) %&gt;% list_rbind() %&gt;%\n  group_by(FPC) %&gt;%\n  summarize(Est = mean(EVal),\n            LB = quantile(EVal, probs = c(0.025)), \n            UB = quantile(EVal, probs = c(0.975))) %&gt;%\n  ggplot(aes(x = FPC, y = Est, group = \"\")) +\n  geom_point() + \n  geom_line() + \n  geom_errorbar(aes(ymin = LB, ymax = UB), width = 0.5) + \n  scale_x_discrete(labels = parse(text = labs)) + \n  labs(y = \"Eigenvalue\") + \n  theme_bw() + \n  theme(axis.title.x = element_blank())\n\n\n\n\n\n\n\n\nThis document just visualizes a particular case of applying the “FAST” approach to perform fully-Bayesian Functional PCA. All supporting STAN and R code here is designed to be flexible, such that it may directly be applied to any other similarly structured problem."
  },
  {
    "objectID": "posts/Visualizing.html",
    "href": "posts/Visualizing.html",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "Cardiovascular-Kidney-Metabolic (CKM) Syndrome Stage\nTroponin T/I\nMinutes of Moderate-to-Vigorous Physical Activity (MVPA)\n\n\nActigraph GT3X [1]"
  },
  {
    "objectID": "posts/Visualizing.html#nhanes-2003-2006",
    "href": "posts/Visualizing.html#nhanes-2003-2006",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "Cardiovascular-Kidney-Metabolic (CKM) Syndrome Stage\nTroponin T/I\nMinutes of Moderate-to-Vigorous Physical Activity (MVPA)\n\n\nActigraph GT3X [1]"
  },
  {
    "objectID": "posts/Visualizing.html#table-1",
    "href": "posts/Visualizing.html#table-1",
    "title": "Data Visualization Principles",
    "section": "“Table 1”",
    "text": "“Table 1”\n\nInitial PassImprovement 1A New PerspectivePrinciples\n\n\nWe use the table1 R package to make a simple, standard table 1:\n\n\nCKMStage 0(N=487)Stage 1(N=1179)Stage 2(N=3963)Stage 3(N=772)Stage 4(N=845)Age (yrs)  Mean (SD)34.1 (12.4)38.9 (14.1)48.5 (15.1)76.7 (7.44)68.8 (12.8)  Median [Min, Max]32.0 [20.0, 85.0]36.0 [20.0, 85.0]48.0 [20.0, 85.0]78.0 [32.0, 85.0]70.0 [26.0, 85.0]Female67.56%55.47%48.25%43.78%42.72%MVPA (min/day)  Mean (SD)27.0 (22.6)21.7 (21.1)18.2 (19.9)6.45 (10.3)7.39 (12.1)  Median [Min, Max]21.9 [0, 187]15.7 [0, 181]11.9 [0, 175]2.43 [0, 83.7]2.43 [0, 107]Diabetes0.00%0.00%12.06%33.42%35.15%Hypertension0.00%0.00%72.55%93.78%94.91%\n\n\n\n\nWe can increase the information density by removing unnecessary information:\n\n\nCKMStage 0(N=487)Stage 1(N=1179)Stage 2(N=3963)Stage 3(N=772)Stage 4(N=845)MVPA (min/day)27.0 (25.0, 29.0)21.7 (20.5, 22.9)18.2 (17.6, 18.8)6.4 (5.7, 7.2)7.4 (6.6, 8.2)Age (yrs)34.1 (33.0, 35.2)38.9 (38.1, 39.7)48.5 (48.1, 49.0)76.7 (76.1, 77.2)68.8 (68.0, 69.7)Female68%55%48%44%43%Diabetes0%0%12%33%35%Hypertension0%0%73%94%95%\n\n\n\n\nIt is easier to perform comparisons vertically, rather than horizontally:\n\n\nCKM StageNMVPAAgeFemaleDiabetesHypertensionStage 048727.0(26.9, 27.1)34[24, 40]68%0%0%Stage 11,17921.7(21.7, 21.7)39[28, 47]55%0%0%Stage 23,96318.2(18.2, 18.2)49[37, 61]48%12%73%Stage 37726.4(6.4, 6.4)77[72, 83]44%33%94%Stage 48457.4(7.4, 7.4)69[61, 80]43%35%95%\n\n\n\n\n\nGrouping by proximity (Gestalt Principle)\nData-ink-ratio (Tufte)\nEncourage comparison (Tufte)\nVertical contrasts are easier (Broman)"
  },
  {
    "objectID": "posts/Visualizing.html#mvpa-by-ckm-status",
    "href": "posts/Visualizing.html#mvpa-by-ckm-status",
    "title": "Data Visualization Principles",
    "section": "MVPA by CKM Status",
    "text": "MVPA by CKM Status\n\nInitial PassImprovement 1Improvement 2Principles\n\n\nWe first leverage a standard bar chart with uncertainty bounds to first visualize the mean minutes of MVPA by CKM status:\n\n\n\n\n\n\n\n\n\n\n\nWe can show a subsample of the actual data, and remove the bar portions which provide no additional information:\n\n\n\n\n\n\n\n\n\n\n\nWe can invite comparison by linking the points together with a line, and scale the right-skewed data with a log transformation:\n\n\n\n\n\n\n\n\n\n\n\n\nShow the data (Tufte)\nData-ink-ratio (Tufte)\nData transforms (Broman)\nGrouping by continuity (Gestalt Principal)"
  },
  {
    "objectID": "posts/Visualizing.html#mvpa-by-ckm-cont.",
    "href": "posts/Visualizing.html#mvpa-by-ckm-cont.",
    "title": "Data Visualization Principles",
    "section": "MVPA by CKM Cont.",
    "text": "MVPA by CKM Cont.\n\nInitial PassImprovement 1Improvement 2Principles\n\n\nHistograms are a common method for visualizing distributions such as that of MVPA within each CKM status:\n\n\n\n\n\n\n\n\n\n\n\nWe cannot really compare the histograms due to the information of interest being along the x-axis and the differing summative frequencies. We can instead vertically stack density plots:\n\n\n\n\n\n\n\n\n\n\n\nThe vertical stack of density plots is still not optimal for comparison, so we can overlay them on one plot with appropriate coloration.\n\n\n\n\n\n\n\n\n\n\n\n\nData density (Tufte)\nSame scales (Broman)\nData transforms (Broman)\nRapid vs deliberate thinking (Kahneman)"
  },
  {
    "objectID": "posts/Visualizing.html#aha-activity-guideline",
    "href": "posts/Visualizing.html#aha-activity-guideline",
    "title": "Data Visualization Principles",
    "section": "AHA Activity Guideline",
    "text": "AHA Activity Guideline\n\nInitial PassImprovement 1Improvement 2Principles\n\n\nTo visualize the CKM status compositions within the subpopulations meeting and not meeting the AHA MVPA guideline, a first thought might be to use pie charts. This makes some sense, as we wish to display proportions within each group:\n\n\n\n\n\n\n\n\n\n\n\nPie charts are notoriously difficult to accurately read, so we can transition to a stacked bar chart:\n\n\n\n\n\n\n\n\n\n\n\nWhile the stacked bar chart is an improvement, the relevant lengths are not juxtaposed to perform the comparison of interest. We can arrange them instead using a standard bar chart:\n\n\n\n\n\n\n\n\n\n\n\n\nEasy comparison (Broman, Tufte, Simkin and Hastie)\nGrouping by proximity (Gestalt Principle)"
  },
  {
    "objectID": "posts/Visualizing.html#mvpa-vs-troponin-t",
    "href": "posts/Visualizing.html#mvpa-vs-troponin-t",
    "title": "Data Visualization Principles",
    "section": "MVPA vs Troponin T",
    "text": "MVPA vs Troponin T\n\nInitial PassImprovementA New PerspectivePrinciples\n\n\nWe first model the expected probability of elevated troponin (by sex-specific cutpoint), adjusting for the known confounder of age.\n\n\n\n\n\n\n\n\n\n\n\nWisualizing expected probability of exceeding this threshold, particularly just using a model, obfuscates the data. Instead we can simply plot troponin vs activity and corresponding smooths, stratified by age:\n\n\n\n\n\n\n\n\n\n\n\nWe want to both understand the marginal distributions of activity and troponin by age group, as well as the association between these two around the AHA recommendation. For these purposes, and to ease comparison, we combine into a single plot with marginal visualizations and a magnification plot:\n\n\n\n\n\n\n\n\n\n\n\n\nLet the data talk (Tufte, Broman)\nRapid AND deliberate elements (Kahneman)\nVisualize multiple scales (Tufte)\nGrouping by enclosure (Gestalt Principle)"
  },
  {
    "objectID": "posts/Visualizing.html#further-reading",
    "href": "posts/Visualizing.html#further-reading",
    "title": "Data Visualization Principles",
    "section": "Further Reading",
    "text": "Further Reading\n\n\n[1] Thornton C, Kolehmainen N, Nazarpour K. Using unsupervised machine learning to quantify physical activity from accelerometry in a diverse and rapidly changing population. PLOS Digital Health 2023;2:e0000220. https://doi.org/10.1371/journal.pdig.0000220.\n\n\n[2] Kahneman D. Attention and effort. Prentice-Hall; 1973.\n\n\n[3] Tufte ER. The visual display of quantitative information. Second. Cheshire, Connecticut: Graphics Press; 2001.\n\n\n[4] What are the Gestalt Principles? — updated 2024. The Interaction Design Foundation n.d.\n\n\n[5] Simkin D, Hastie R. An Information-Processing Analysis of Graph Perception. Journal of the American Statistical Association 1987;82:454–65. https://doi.org/10.1080/01621459.1987.10478448.\n\n\n[6] Broman K. Advanced data analysis 2020.\n\n\n[7] Padilla LM, Creem-Regehr SH, Hegarty M, Stefanucci JK. Decision making with visualizations: A cognitive framework across disciplines. Cognitive Research: Principles and Implications 2018;3:29. https://doi.org/10.1186/s41235-018-0120-9."
  },
  {
    "objectID": "resources/publications/pre_print/matabuena_multilevel_2024.html",
    "href": "resources/publications/pre_print/matabuena_multilevel_2024.html",
    "title": "Multilevel functional data analysis modeling of human glucose response to meal intake",
    "section": "",
    "text": "Matabuena, M., & Sartini, J. (2024). Multilevel functional data analysis modeling of human glucose response to meal intake. ArXiv."
  },
  {
    "objectID": "resources/publications/pre_print/matabuena_multilevel_2024.html#citation-apa",
    "href": "resources/publications/pre_print/matabuena_multilevel_2024.html#citation-apa",
    "title": "Multilevel functional data analysis modeling of human glucose response to meal intake",
    "section": "",
    "text": "Matabuena, M., & Sartini, J. (2024). Multilevel functional data analysis modeling of human glucose response to meal intake. ArXiv."
  },
  {
    "objectID": "resources/publications/pre_print/matabuena_multilevel_2024.html#abstract",
    "href": "resources/publications/pre_print/matabuena_multilevel_2024.html#abstract",
    "title": "Multilevel functional data analysis modeling of human glucose response to meal intake",
    "section": "Abstract",
    "text": "Abstract\nGlucose meal response information collected via Continuous Glucose Monitoring (CGM) is relevant to the assessment of individual metabolic status and the support of personalized diet prescriptions. However, the complexity of the data produced by CGM monitors pushes the limits of existing analytic methods. CGM data often exhibits substantial within-person variability and has a natural multilevel structure. This research is motivated by the analysis of CGM data from individuals without diabetes in the AEGIS study. The dataset includes detailed information on meal timing and nutrition for each individual over different days. The primary focus of this study is to examine CGM glucose responses following patients’ meals and explore the time-dependent associations with dietary and patient characteristics. Motivated by this problem, we propose a new analytical framework based on multilevel functional models, including a new functional mixed R-square coefficient. The use of these models illustrates 3 key points: (i) The importance of analyzing glucose responses across the entire functional domain when making diet recommendations; (ii) The differential metabolic responses between normoglycemic and prediabetic patients, particularly with regards to lipid intake; (iii) The importance of including random, person-level effects when modelling this scientific problem."
  },
  {
    "objectID": "resources/publications/pre_print/sartini_BFPCA_2024.html",
    "href": "resources/publications/pre_print/sartini_BFPCA_2024.html",
    "title": "Fast Bayesian Functional Principal Components Analysis",
    "section": "",
    "text": "Sartini, J., Zhou, X., Selvin, L., Zeger, S., & Crainiceanu, C. (2025). Fast Bayesian Functional Principal Components Analysis (No. arXiv:2412.11340). arXiv. https://doi.org/10.48550/arXiv.2412.11340"
  },
  {
    "objectID": "resources/publications/pre_print/sartini_BFPCA_2024.html#citation-apa-7",
    "href": "resources/publications/pre_print/sartini_BFPCA_2024.html#citation-apa-7",
    "title": "Fast Bayesian Functional Principal Components Analysis",
    "section": "",
    "text": "Sartini, J., Zhou, X., Selvin, L., Zeger, S., & Crainiceanu, C. (2025). Fast Bayesian Functional Principal Components Analysis (No. arXiv:2412.11340). arXiv. https://doi.org/10.48550/arXiv.2412.11340"
  },
  {
    "objectID": "resources/publications/pre_print/sartini_BFPCA_2024.html#abstract",
    "href": "resources/publications/pre_print/sartini_BFPCA_2024.html#abstract",
    "title": "Fast Bayesian Functional Principal Components Analysis",
    "section": "Abstract",
    "text": "Abstract\nFunctional Principal Components Analysis (FPCA) is a widely used analytic tool for dimension reduction of functional data. Traditional implementations of FPCA estimate the principal components from the data, then treat these estimates as fixed in subsequent analyses. To account for the uncertainty of PC estimates, we propose FAST, a fully-Bayesian FPCA with three core components: (1) projection of eigenfunctions onto an orthonormal spline basis; (2) efficient sampling of the orthonormal spline coefficient matrix using polar decomposition; and (3) ordering eigenvalues during sampling. Extensive simulation studies show that FAST is very stable and performs better compared to existing methods. FAST is motivated by and applied to a study of the variability in mealtime glucose from the Dietary Approaches to Stop Hypertension for Diabetes Continuous Glucose Monitoring (DASH4D CGM) study. All relevant STAN code and simulation routines are available as supplementary material."
  },
  {
    "objectID": "resources/publications/in_print/michelmata_evolution_2024.html",
    "href": "resources/publications/in_print/michelmata_evolution_2024.html",
    "title": "The evolution of private reputations in information-abundant landscapes",
    "section": "",
    "text": "Michel-Mata, S., Kawakatsu, M., Sartini, J. et al. The evolution of private reputations in information-abundant landscapes. Nature 634, 883–889 (2024). https://doi.org/10.1038/s41586-024-07977-x"
  },
  {
    "objectID": "resources/publications/in_print/michelmata_evolution_2024.html#citation-apa",
    "href": "resources/publications/in_print/michelmata_evolution_2024.html#citation-apa",
    "title": "The evolution of private reputations in information-abundant landscapes",
    "section": "",
    "text": "Michel-Mata, S., Kawakatsu, M., Sartini, J. et al. The evolution of private reputations in information-abundant landscapes. Nature 634, 883–889 (2024). https://doi.org/10.1038/s41586-024-07977-x"
  },
  {
    "objectID": "resources/publications/in_print/michelmata_evolution_2024.html#abstract",
    "href": "resources/publications/in_print/michelmata_evolution_2024.html#abstract",
    "title": "The evolution of private reputations in information-abundant landscapes",
    "section": "Abstract",
    "text": "Abstract\nReputations are critical to human societies, as individuals are treated differently based on their social standing1,2. For instance, those who garner a good reputation by helping others are more likely to be rewarded by third parties3,4,5. Achieving widespread cooperation in this way requires that reputations accurately reflect behaviour6 and that individuals agree about each other’s standings7. With few exceptions8,9,10, theoretical work has assumed that information is limited, which hinders consensus7,11 unless there are mechanisms to enforce agreement, such as empathy12, gossip13,14,15 or public institutions16. Such mechanisms face challenges in a world where empathy, effective communication and institutional trust are compromised17,18,19. However, information about others is now abundant and readily available, particularly through social media. Here we demonstrate that assigning private reputations by aggregating several observations of an individual can accurately capture behaviour, foster emergent agreement without enforcement mechanisms and maintain cooperation, provided individuals exhibit some tolerance for bad actions. This finding holds for both first- and second-order norms of judgement and is robust even when norms vary within a population. When the aggregation rule itself can evolve, selection indeed favours the use of several observations and tolerant judgements. Nonetheless, even when information is freely accessible, individuals do not typically evolve to use all of it. This method of assessing reputations—‘look twice, forgive once’, in a nutshell—is simple enough to have arisen early in human culture and powerful enough to persist as a fundamental component of social heuristics."
  },
  {
    "objectID": "resources/publications/in_print/selvin_associations_2023.html",
    "href": "resources/publications/in_print/selvin_associations_2023.html",
    "title": "The Associations of Mean Glucose and Time in Range from Continuous Glucose Monitoring with HbA1c in Adults with Type 2 Diabetes",
    "section": "",
    "text": "Selvin, E., Wang, D., Rooney, M. R., Echouffo-Tcheugui, J., Fang, M., Zeger, S., Sartini, J., Tang, O., Coresh, J., Aurora, R. N., & Punjabi, N. M. (2023). The Associations of Mean Glucose and Time in Range from Continuous Glucose Monitoring with HbA1c in Adults with Type 2 Diabetes. Diabetes technology & therapeutics, 25(1), 86–90. https://doi.org/10.1089/dia.2022.0178"
  },
  {
    "objectID": "resources/publications/in_print/selvin_associations_2023.html#citation-apa-7",
    "href": "resources/publications/in_print/selvin_associations_2023.html#citation-apa-7",
    "title": "The Associations of Mean Glucose and Time in Range from Continuous Glucose Monitoring with HbA1c in Adults with Type 2 Diabetes",
    "section": "",
    "text": "Selvin, E., Wang, D., Rooney, M. R., Echouffo-Tcheugui, J., Fang, M., Zeger, S., Sartini, J., Tang, O., Coresh, J., Aurora, R. N., & Punjabi, N. M. (2023). The Associations of Mean Glucose and Time in Range from Continuous Glucose Monitoring with HbA1c in Adults with Type 2 Diabetes. Diabetes technology & therapeutics, 25(1), 86–90. https://doi.org/10.1089/dia.2022.0178"
  },
  {
    "objectID": "resources/publications/in_print/selvin_associations_2023.html#abstract",
    "href": "resources/publications/in_print/selvin_associations_2023.html#abstract",
    "title": "The Associations of Mean Glucose and Time in Range from Continuous Glucose Monitoring with HbA1c in Adults with Type 2 Diabetes",
    "section": "Abstract",
    "text": "Abstract\nAssociations of mean glucose and time in range (70–180 mg/dL) from continuous glucose monitoring (CGM) with HbA1c in adults with type 2 diabetes are not well characterized. We conducted a secondary analysis of 186 participants from the Hyperglycemic Profiles in Obstructive Sleep Apnea (HYPNOS) trial. Participants simultaneously wore Dexcom G4 and Abbott Libre Pro CGM sensors up to 4 weeks. Mean HbA1c was 7.7% (SD, 1.3). There were strong negative Pearson’s correlations of HbA1c with CGM time in range (−0.79, Abbott; −0.81, Dexcom) and strong positive correlations with CGM mean glucose (Dexcom, 0.84; Abbott, 0.82). However, there were large differences in CGM mean glucose (±20 mg/dL) and time in range (±14%) at any given HbA1c value. Mean glucose and HbA1c are strongly correlated in type 2 diabetes patients not taking insulin but discordance is evident at the individual level. Clinicians should expect discordance and use HbA1c and CGM in a complementary manner. ClinicalTrials.gov Identifier: NCT02454153"
  },
  {
    "objectID": "cv_host.html",
    "href": "cv_host.html",
    "title": "Sartini-Stats",
    "section": "",
    "text": "Download PDF File\n   \n    Unable to display PDF file. Download instead."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Johns Hopkins University - Bloomberg SPH | August 2021 - present\n\n\n\nPrinceton University | August 2017 - May 2021"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "Johns Hopkins University - Bloomberg SPH | August 2021 - present\n\n\n\nPrinceton University | August 2017 - May 2021"
  },
  {
    "objectID": "about.html#research-groups",
    "href": "about.html#research-groups",
    "title": "About Me",
    "section": "Research Groups",
    "text": "Research Groups\n\nJohns Hopkins University\n\nWearable and Implantable Technology (WIT) Working Group | December 2022 - present\nDiabetes Data Group | August 2022 - present\nZeger Lab | August 2022 - present\n\n\n\n\nPresentation of Applied Research at AHA Epi/Lifestyle 2025\n\n\n\n\nPrinceton University\n\nIndirect Reciprocity Working Group | November 2019 - August 2021\nTransportation Systems Lab | May 2020 - September 2020\n\n\n\nUniversity of Arkansas\n\nSystems Engineering Lab | May 2020 - May 2021"
  },
  {
    "objectID": "about.html#teaching-experience",
    "href": "about.html#teaching-experience",
    "title": "About Me",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nJohns Hopkins University\n\nAnalysis of Longitudinal and Multilevel Data Lead TA | Fall 2025\nPrivate Tutoring in Statistical Methods and Theory | May 2023 - present\nStatistical Computing and Machine Learning TA | August 2022 - May 2025\nTA Training Planning Committee | 2023-2025\n\n\n\n\nGuest Lecture to Undergraduate Biostatistics Students Covering my Research\n\n\n\n\nPrinceton Univeristy\n\nStatistical Courses TA | August 2020 - May 2021"
  },
  {
    "objectID": "about.html#honors-and-awards",
    "href": "about.html#honors-and-awards",
    "title": "About Me",
    "section": "Honors and Awards",
    "text": "Honors and Awards\n\nLouis I. and Thomas D. Dublin Award for the Advancement of Epidemiology and Biostatistics | 2025\nHelen Abbey Award for Commitment to Teaching | 2024\nAhmet S. Çakmak Prize for Innovative Research | 2021\nGeorge J. Mueller Award for High Scholarly Achievement and Quality Athletic Performance | 2021"
  },
  {
    "objectID": "about.html#software-experience",
    "href": "about.html#software-experience",
    "title": "About Me",
    "section": "Software Experience",
    "text": "Software Experience\nSoftware Engineering Intern | PrivacyStar | May 2019 - August 2019"
  },
  {
    "objectID": "soft.html",
    "href": "soft.html",
    "title": "Software",
    "section": "",
    "text": "MSFAST Bayesian Multivariate Sparse FPCA | Supporting material for the manuscript introducing the MSFAST approach to Bayesian Functional Principal Components Analysis for multivariate, sparsely-observed data. Includes STAN and R codes providing univariate and multivariate sparse data simulations comparing MSFAST with existing implementations, evaluating estimation accuracy and inference validity. Additionally includes a vignette illustrating a real analysis on the CONTENT child growth data.\nFAST Bayesian FPCA | Supporting material for the manuscript introducing the FAST approach to fitting Bayesian Functional Principal Components Analysis. Includes STAN and R codes providing simulation and model fitting routines comparing FAST with existing implementations on a simple simulation scenario and a multilevel scenario, evaluating estimation accuracy and inference validity.\nrGCI | Supporting R software for calculation of the novel Glucose Color Index (GCI) frequency-domain summary of Continuous Glucose Monitoring Data. Includes tools for calculating the smoothed log-periodogram using two weeks of glucose data, summarizing this function into 6 measures using a piece-wise linear model, and finally producing the GCI using weights derived via canonical correlation analysis."
  }
]